<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions</title>
  <meta name="description" content="2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions" />
  
  
  

<meta name="author" content="Tyler Farr" />


<meta name="date" content="2024-07-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="the-pool-of-tears.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">March Madness Preds</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> March Madness Matchup Predictions</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#methodology"><i class="fa fa-check"></i><b>1.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>1.3</b> Exploratory Data Analysis (EDA)</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#modeling"><i class="fa fa-check"></i><b>1.4</b> Modeling</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#model-comparison"><i class="fa fa-check"></i><b>1.5</b> Model Comparison</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#results"><i class="fa fa-check"></i><b>1.6</b> Results</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#areas-of-improvement"><i class="fa fa-check"></i><b>1.7</b> Areas of Improvement</a></li>
<li class="chapter" data-level="1.8" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.8</b> References</a></li>
<li class="chapter" data-level="1.9" data-path="index.html"><a href="index.html#additional-code-appendix"><i class="fa fa-check"></i><b>1.9</b> Additional Code Appendix</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-pool-of-tears.html"><a href="the-pool-of-tears.html"><i class="fa fa-check"></i><b>2</b> The pool of tears</a></li>
<li class="chapter" data-level="3" data-path="a-caucus-race-and-a-long-tale.html"><a href="a-caucus-race-and-a-long-tale.html"><i class="fa fa-check"></i><b>3</b> A caucus-race and a long tale</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">2024 Men’s March Madness NCAA Basketball Tournament Matchup Predictions</h1>
<p class="author"><em>Tyler Farr</em></p>
<p class="date"><em>2024-07-04</em></p>
</div>
<div id="march-madness-matchup-predictions" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> March Madness Matchup Predictions<a href="index.html#march-madness-matchup-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction<a href="index.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Objective</strong></p>
<p><em>Predict winners of the 2024 Men’s NCAA March Madness tournament matchups through various machine learning models (logistic regression, random forest, extreme gradient boosting (XGBoost), support vector machines (SVM), Naive Bayes, and multivariate adaptive regression splines (MARS))</em></p>
<ul>
<li><p>The MARS model performed the best on the test dataset and finished with an <strong>ESPN score of 870 (69% accuracy) and 3/8 Elite 8 teams correct for the 2024 tournament.</strong></p></li>
<li><p>The XGBoost model performed the next best on the test dataset and finished with an <strong>ESPN score of 360 (12.9% accuracy) and 1/8 Elite 8 teams correct for the 2024 tournament.</strong></p></li>
</ul>
<p>The XGBoost model heavily favored upsets and while it only had 1 of the 8 Elite 8 teams, it did correctly pick NC State to be there. Please see the <strong>Results and Recommendations</strong> section for a more detailed analysis of these model performances.</p>
<p><strong>Background</strong></p>
<p>The NCAA March Madness tournament is an annual basketball tournament held every March. 64 teams compete against each other in a single game elimination, bracket style tournament. Individuals can build predictions on who they think will win for each possible matchup and how the bracket will turn out for each round of the March Madness tournament (Round of 64, Round of 32, Sweet 16, Elite 8, Final 4, and Championship). The odds of a perfect bracket are 1 in 9.2 quintillion, if you chose winners at random.</p>
<p>Consequentially, I was interested in using data science skills and machine learning models to help me in predicting the winners of these brackets. While I have no expectations of creating a perfect bracket, the statistics on team performance throughout the year provide data backed decisions on who will win certain matchups in the NCAA tournament.</p>
<p><em>How are points calculated for March Madness brackets in the ESPN Tournament Challenge?</em></p>
<ul>
<li><p><strong>Round of 64</strong> = 10 points per correct pick (32 games)</p></li>
<li><p><strong>Round of 32</strong> = 20 points per correct pick (16 games)</p></li>
<li><p><strong>Sweet 16</strong> = 40 points per correct pick (8 games)</p></li>
<li><p><strong>Elite 8</strong> = 80 points per correct pick (4 games)</p></li>
<li><p><strong>Final 4</strong> = 160 points per correct pick (2 games)</p></li>
<li><p><strong>Championship</strong> = 320 points per correct pick (1 game)</p></li>
</ul>
<p>The maximum possible points you can earn for a perfect bracket is 1920 points. In general, I would say most of my brackets in the past have scored around 400 - 900 points. Winning a large bracket pool usually will take a score of least 1000 points (picking the champion or some of the final 4 teams), but obviously depends on how the pool plays out each year.</p>
<p>It’s extremely important to have your Elite 8 and Final 4 teams still alive after the first two rounds of the tournament because that keeps you in the running for the maximum points possible. The best bracket I have ever created was for the 2017 March Madness tournament, where I picked UNC to win the whole tournament and (luckily) chose South Carolina to be in the Final 4. This bracket scored a 1630.</p>
<p><strong>Data</strong></p>
<p>The data collected was from the 2008 to 2023 March Madness tournaments on each game outcome. There are 63 games every tournaments, resulting in 944 rows of all tournament matchups in this time frame (1 game was cancelled in 2021 due to COVID-19). I chose to start in 2008 due to availability of the data as well as I wanted to use data that relevantly reflected the style of play of today’s game.</p>
<p>This data included:</p>
<ul>
<li><p>Year</p></li>
<li><p>TeamA and TeamB</p></li>
<li><p>Tournament Seed</p></li>
<li><p>Final score of the matchup</p></li>
</ul>
<p>Individual team data was web-scraped from TeamRankings.com, Bart Torvik, and SportsReference.com (College Basketball). Please check the references section for a full list of the data sources, as well as web-scraping code.</p>
<p>This data included:</p>
<ul>
<li><p>Team performance metrics and averages</p>
<ul>
<li>Field Goal %</li>
<li>3-point %</li>
<li>Free Throw %</li>
<li>Steals</li>
<li>Assists</li>
<li>Blocks</li>
<li>Turnovers</li>
<li>Personal fouls</li>
<li>Total rebounds</li>
<li>Offensive rebounds</li>
<li>Opponent points per game (PPG)</li>
<li>Opponent turnovers</li>
<li>Win/Loss %</li>
<li>Effective FG%</li>
<li>Average Margin of Victory (MOV)</li>
<li>Win/Loss % (past 10 games)</li>
</ul></li>
<li><p>Advanced metrics</p>
<ul>
<li>Offensive/Defensive efficiency (unadjusted)</li>
<li>Pace of play</li>
<li>Extra scoring chances</li>
<li>Ratings percentage index (RPI)</li>
</ul></li>
<li><p>Bart Torvik formulas</p>
<ul>
<li>Offensive/Defensive efficiency (adjusted)</li>
<li>Pace of play (adjusted)</li>
<li>Wins above bubble (WAB - how many games a team won relative to a bubble-quality team)</li>
<li>BARTHAG (overall team evaluation metric)</li>
</ul></li>
<li><p>Feature Engineered Variables I created</p>
<ul>
<li>Pythagorean Win Expectation</li>
<li>Luck</li>
<li>Pace (adjusted) / team efficiency (adjusted)</li>
<li>Opponent turnovers / team turnovers</li>
</ul></li>
</ul>
</div>
<div id="methodology" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Methodology<a href="index.html#methodology" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here are the libraries I used to create this project:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="index.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="index.html#cb1-2" tabindex="-1"></a><span class="fu">library</span>(bookdown)</span>
<span id="cb1-3"><a href="index.html#cb1-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="index.html#cb1-4" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb1-5"><a href="index.html#cb1-5" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb1-6"><a href="index.html#cb1-6" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb1-7"><a href="index.html#cb1-7" tabindex="-1"></a><span class="fu">library</span>(readxl)</span>
<span id="cb1-8"><a href="index.html#cb1-8" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb1-9"><a href="index.html#cb1-9" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb1-10"><a href="index.html#cb1-10" tabindex="-1"></a><span class="fu">library</span>(nortest)</span>
<span id="cb1-11"><a href="index.html#cb1-11" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-12"><a href="index.html#cb1-12" tabindex="-1"></a><span class="fu">library</span>(forcats)</span>
<span id="cb1-13"><a href="index.html#cb1-13" tabindex="-1"></a><span class="fu">library</span>(gmodels)</span>
<span id="cb1-14"><a href="index.html#cb1-14" tabindex="-1"></a><span class="fu">library</span>(vcdExtra)</span>
<span id="cb1-15"><a href="index.html#cb1-15" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span>
<span id="cb1-16"><a href="index.html#cb1-16" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb1-17"><a href="index.html#cb1-17" tabindex="-1"></a><span class="fu">library</span>(ROCit)</span>
<span id="cb1-18"><a href="index.html#cb1-18" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb1-19"><a href="index.html#cb1-19" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb1-20"><a href="index.html#cb1-20" tabindex="-1"></a><span class="fu">library</span>(smbinning)</span>
<span id="cb1-21"><a href="index.html#cb1-21" tabindex="-1"></a><span class="fu">library</span>(Ckmeans<span class="fl">.1</span>d.dp)</span>
<span id="cb1-22"><a href="index.html#cb1-22" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb1-23"><a href="index.html#cb1-23" tabindex="-1"></a><span class="fu">library</span>(klaR)</span>
<span id="cb1-24"><a href="index.html#cb1-24" tabindex="-1"></a><span class="fu">library</span>(earth)</span>
<span id="cb1-25"><a href="index.html#cb1-25" tabindex="-1"></a><span class="fu">library</span>(bookdown)</span>
<span id="cb1-26"><a href="index.html#cb1-26" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span></code></pre></div>
<p>Next, I load in the dataset. Please see <a href="https://github.com/tylerfarr5/march-madness" class="uri">https://github.com/tylerfarr5/march-madness</a> for all associated data files. I chose to start by original dataset at 2008 because certain variables that I wanted to use in the model were not available until the start of the 2008 season. Additionally, one could make an argument that the game of college basketball in 2024 is not the same as it was in 2000, so the 2008 data will at least be more relevant to the game today.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="index.html#cb2-1" tabindex="-1"></a>mm_data <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;march_madness.xlsx&quot;</span>, <span class="at">sheet =</span> <span class="dv">2</span>)</span>
<span id="cb2-2"><a href="index.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="index.html#cb2-3" tabindex="-1"></a><span class="co">#only want 2008-2023</span></span>
<span id="cb2-4"><a href="index.html#cb2-4" tabindex="-1"></a>mm_data <span class="ot">&lt;-</span> mm_data <span class="sc">%&gt;%</span></span>
<span id="cb2-5"><a href="index.html#cb2-5" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(<span class="sc">!</span>(Year <span class="sc">%in%</span> <span class="dv">2000</span><span class="sc">:</span><span class="dv">2007</span>))</span></code></pre></div>
<p>A quick note about the data - I chose to compile many different data sources into one file, so I found it easier to web-scrape and compile everything in Microsoft Excel first, before reading that file R. A quick snapshot of what the data frame looked like is provided below:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="index.html#cb3-1" tabindex="-1"></a><span class="fu">head</span>(mm_data)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 44
##    Year TeamA TeamB A_Seed B_Seed A_Score B_Score   MOV Winner_A Round Diff_FGPct_PRE
##   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;
## 1  2008 UNC   Moun…      1     16     113      74    39        1 Roun…        0.0390 
## 2  2008 Indi… Arka…      8      9      72      86   -14        0 Roun…       -0.00300
## 3  2008 Notr… Geor…      5     12      68      50    18        1 Roun…       -0.0100 
## 4  2008 Wash… Wint…      4     13      71      40    31        1 Roun…        0.0380 
## 5  2008 Okla… St. …      6     11      72      64     8        1 Roun…       -0.0350 
## 6  2008 Loui… Bois…      3     14      79      61    18        1 Roun…       -0.049  
## # ℹ 33 more variables: Diff_3ptPct_PRE &lt;dbl&gt;, Diff_FTPct_PRE &lt;dbl&gt;,
## #   Diff_TRB_PRE &lt;dbl&gt;, Diff_AST_PRE &lt;dbl&gt;, Diff_STL_PRE &lt;dbl&gt;, Diff_BLK_PRE &lt;dbl&gt;,
## #   Diff_TO_PRE &lt;dbl&gt;, Diff_PF_PRE &lt;dbl&gt;, Diff_PPG_PRE &lt;dbl&gt;,
## #   Diff_OppFGPct_PRE &lt;dbl&gt;, Diff_OppPPG_PRE &lt;dbl&gt;, Diff_UnadjOffEff_PRE &lt;dbl&gt;,
## #   Diff_UnadjDefEff_PRE &lt;dbl&gt;, Diff_WLPct_PRE &lt;dbl&gt;, Diff_Pace_PRE &lt;dbl&gt;,
## #   Diff_eFGPct_PRE &lt;dbl&gt;, Diff_OReb_PRE &lt;dbl&gt;, Diff_AvgMOV_PRE &lt;dbl&gt;,
## #   Diff_ExtraScoringChancesPG_PRE &lt;dbl&gt;, Diff_PythagoreanExp_PRE &lt;dbl&gt;, …</code></pre>
<p><strong>Matchups</strong></p>
<p>Here are some of the rules in place on matchup data in the dataset:</p>
<ul>
<li><p>The higher seed will always be TeamA and the lower seed will be TeamB</p></li>
<li><p>If a matchup has two seeds that are the same number, then team with higher points per game will be TeamA</p></li>
<li><p>I differenced each teams’ season statistics to evaluate how good or bad a team is relative to their opponent, and then used the differenced values as predictors in the model. The difference was TeamA minus TeamB.</p></li>
<li><p>Since a final score was provided from each game, that left an opportunity for a continuous target variable of the margin of victory (MOV), or a binary target variable for whether or not TeamA won the game. I chose to model the binary target variable for whether or not TeamA won.</p></li>
</ul>
<p><strong>Pre and Post-Tourney Metrics</strong></p>
<p>I had originally pulled my data from SportsReference, however I realized that these team statistics reflect post-NCAA tournament numbers. In order to build and evaluate the model on data that is most reflective of team performance, I needed to get data that was pre-tournament. For example, if Clemson were to get hot and start making 3-point shots throughout the March Madness tournament, that would likely drive up their season 3-point shooting percentage and is different from where they stood before the tournament.</p>
<p>Before fully committing to using pre-tournament numbers, I performed paired t-tests (Wilcoxon signed rank test) and tests of normality (Shapiro-Wilks test) to evaluate pre and post tournament performance on a variety of team metrics. For some metrics, like a team’s median field goal percentage, did not have a statistically significant different before and after the tournament. However, metrics like total rebounds, win-loss percentage, turnovers, and many more were statistically different. These results led me to conclude that I needed to use pre-tournament numbers. An example of how I did this is provided below.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="index.html#cb5-1" tabindex="-1"></a><span class="co">#Team FG% - no statistical difference</span></span>
<span id="cb5-2"><a href="index.html#cb5-2" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">with</span>(mm_data, mm_data<span class="sc">$</span>Diff_FGPct <span class="sc">-</span> mm_data<span class="sc">$</span>Diff_FGPct_PRE))</span>
<span id="cb5-3"><a href="index.html#cb5-3" tabindex="-1"></a><span class="fu">wilcox.test</span>(mm_data<span class="sc">$</span>Diff_FGPct, mm_data<span class="sc">$</span>Diff_FGPct_PRE, <span class="at">paired =</span> <span class="cn">TRUE</span>, <span class="at">alternative =</span> <span class="st">&#39;two.sided&#39;</span>)</span>
<span id="cb5-4"><a href="index.html#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="index.html#cb5-5" tabindex="-1"></a><span class="co">#Team Total Rebounds - statistically significant difference between pre and post-tournament numbers</span></span>
<span id="cb5-6"><a href="index.html#cb5-6" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">with</span>(mm_data, mm_data<span class="sc">$</span>Diff_TRB <span class="sc">-</span> mm_data<span class="sc">$</span>Diff_TRB_PRE))</span>
<span id="cb5-7"><a href="index.html#cb5-7" tabindex="-1"></a><span class="fu">wilcox.test</span>(mm_data<span class="sc">$</span>Diff_TRB, mm_data<span class="sc">$</span>Diff_TRB_PRE, <span class="at">paired =</span> <span class="cn">TRUE</span>, <span class="at">alternative =</span> <span class="st">&#39;two.sided&#39;</span>)</span></code></pre></div>
<p>There was no missing data in the dataset.</p>
<p>Before exploring the data, I did a train/test split of the 2008 to 2023 tournaments data. I chose to use the 2023 tournament as my test dataset and evaluated how well the data built on 2008 to 2022 performed on the 2023 bracket. The training set is 881 rows and the testing set is 63 rows.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="index.html#cb6-1" tabindex="-1"></a>train <span class="ot">&lt;-</span> mm_data[mm_data<span class="sc">$</span>Year <span class="sc">!=</span> <span class="dv">2023</span>,] <span class="co">#2000 - 2022 tournaments</span></span>
<span id="cb6-2"><a href="index.html#cb6-2" tabindex="-1"></a>valid <span class="ot">&lt;-</span> mm_data[mm_data<span class="sc">$</span>Year <span class="sc">==</span> <span class="dv">2023</span>,] <span class="co">#2023 tournament</span></span>
<span id="cb6-3"><a href="index.html#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="index.html#cb6-4" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(train) <span class="co">#this helps SMBinning work</span></span>
<span id="cb6-5"><a href="index.html#cb6-5" tabindex="-1"></a>valid <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(valid) <span class="co">#this helps SMBinning work</span></span></code></pre></div>
<pre><code>## [1] &quot;Train Data (2008-2022 brackets) - Rows: 881&quot;</code></pre>
<pre><code>## [1] &quot;Test Data (2023 bracket) - Rows: 63&quot;</code></pre>
<p><strong>Model Evaluation</strong></p>
<p>I chose to evaluate my models using area under the receiver operating characteristic (AUROC) curve, which balances sensitivity and specificity. However, since the best bracket is typically the one that scores the most points in the ESPN Tournament Challenge, I also evaluated how well my model scored in terms of ESPN points.</p>
<p>To do this, I generated every possible matchup for the 2023 tournament, so I could evaluate what the model chose in later rounds where it may have picked the wrong team to win in earlier rounds of the tournament. Here, I read in that dataset:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="index.html#cb9-1" tabindex="-1"></a>conditional2023 <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;march_madness.xlsx&quot;</span>, <span class="at">sheet =</span> <span class="dv">5</span>)</span>
<span id="cb9-2"><a href="index.html#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="index.html#cb9-3" tabindex="-1"></a><span class="fu">head</span>(conditional2023)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 37
##    Year TeamA   TeamB     A_Seed B_Seed Diff_FGPct_PRE Diff_3ptPct_PRE Diff_FTPct_PRE
##   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;
## 1  2023 Alabama Maryland       1      8       -0.00300         0.00800        -0.0130
## 2  2023 Alabama San Dieg…      1      5        0.00500        -0.00600        -0.0100
## 3  2023 Alabama Virginia       1      4       -0.00300        -0.0150          0.0250
## 4  2023 Alabama Creighton      1      6       -0.0210         -0.0220         -0.0410
## 5  2023 Alabama Baylor         1      3       -0.00100        -0.0340         -0.0230
## 6  2023 Alabama Missouri       1      7       -0.0270         -0.0230         -0.0320
## # ℹ 29 more variables: Diff_TRB_PRE &lt;dbl&gt;, Diff_AST_PRE &lt;dbl&gt;, Diff_STL_PRE &lt;dbl&gt;,
## #   Diff_BLK_PRE &lt;dbl&gt;, Diff_TO_PRE &lt;dbl&gt;, Diff_PF_PRE &lt;dbl&gt;, Diff_PPG_PRE &lt;dbl&gt;,
## #   Diff_OppFGPct_PRE &lt;dbl&gt;, Diff_OppPPG_PRE &lt;dbl&gt;, Diff_UnadjOffEff_PRE &lt;dbl&gt;,
## #   Diff_UnadjDefEff_PRE &lt;dbl&gt;, Diff_WLPct_PRE &lt;dbl&gt;, Diff_Pace_PRE &lt;dbl&gt;,
## #   Diff_eFGPct_PRE &lt;dbl&gt;, Diff_OReb_PRE &lt;dbl&gt;, Diff_AvgMOV_PRE &lt;dbl&gt;,
## #   Diff_ExtraScoringChancesPG_PRE &lt;dbl&gt;, Diff_PythagoreanExp_PRE &lt;dbl&gt;,
## #   Diff_Luck_PRE &lt;dbl&gt;, Diff_WLPast10_PRE &lt;dbl&gt;, Diff_AdjO_PRE &lt;dbl&gt;, …</code></pre>
</div>
<div id="exploratory-data-analysis-eda" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Exploratory Data Analysis (EDA)<a href="index.html#exploratory-data-analysis-eda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here, I dive a little deeper into various relationships between the variables in the training data before I build a model.</p>
<p>We start by looking at a correlation matrix of the differenced variables in the model. For generic team basketball metrics, I abbreviated them accordingly. As expected, we see strong correlations between teams offensive efficiencies and their points per game, pace of play, or average margin of victory. One interesting finding was a strong negative correlation between a team’s defensive efficiency and their pythagorean expectation.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="index.html#cb11-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">cor</span>(train[<span class="fu">c</span>(<span class="dv">11</span><span class="sc">:</span><span class="dv">44</span>)])</span>
<span id="cb11-2"><a href="index.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="index.html#cb11-3" tabindex="-1"></a><span class="fu">colnames</span>(res) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;FG% &quot;</span>, <span class="st">&quot;3pt% &quot;</span>, <span class="st">&quot;FT% &quot;</span>, <span class="st">&quot;TRB &quot;</span>, <span class="st">&quot;AST &quot;</span>, <span class="st">&quot;STL &quot;</span>, <span class="st">&quot;BLK &quot;</span>, <span class="st">&quot;TO &quot;</span>, <span class="st">&quot;PF &quot;</span>, <span class="st">&quot;PPG &quot;</span>, <span class="st">&quot;OppFG% &quot;</span>, <span class="st">&quot;OppPPG &quot;</span>, <span class="st">&quot;Unadjusted Off Efficiency &quot;</span>, <span class="st">&quot;Unadjusted Def Efficiency &quot;</span>, <span class="st">&quot;WL% &quot;</span>, <span class="st">&quot;Pace &quot;</span>, <span class="st">&quot;eFG% &quot;</span>, <span class="st">&quot;OReb &quot;</span>, <span class="st">&quot;AvgMOV &quot;</span>, <span class="st">&quot;Extra Scoring ChancesPG &quot;</span>, <span class="st">&quot;Pythagorean Expectation &quot;</span>, <span class="st">&quot;Luck &quot;</span>, <span class="st">&quot;RPI &quot;</span>, <span class="st">&quot;RPI Strength of Schedule &quot;</span>, <span class="st">&quot;WL% Past 10 &quot;</span>, <span class="st">&quot;Adjusted Off Efficiency &quot;</span>, <span class="st">&quot;Adjusted Def Efficiency &quot;</span>, <span class="st">&quot;Adjusted Efficiency Margin &quot;</span>, <span class="st">&quot;BARTHAG &quot;</span>, <span class="st">&quot;Adjusted Tempo &quot;</span>, <span class="st">&quot;Wins Above Bubble &quot;</span>, <span class="st">&quot;OppTO &quot;</span>, <span class="st">&quot;Adjusted Tempo/Adjusted Efficiency Margin Ratio &quot;</span>, <span class="st">&quot;OppTO/TO Ratio &quot;</span>)</span>
<span id="cb11-4"><a href="index.html#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="index.html#cb11-5" tabindex="-1"></a><span class="fu">rownames</span>(res) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;FG% &quot;</span>, <span class="st">&quot;3pt% &quot;</span>, <span class="st">&quot;FT% &quot;</span>, <span class="st">&quot;TRB &quot;</span>, <span class="st">&quot;AST &quot;</span>, <span class="st">&quot;STL &quot;</span>, <span class="st">&quot;BLK &quot;</span>, <span class="st">&quot;TO &quot;</span>, <span class="st">&quot;PF &quot;</span>, <span class="st">&quot;PPG &quot;</span>, <span class="st">&quot;OppFG% &quot;</span>, <span class="st">&quot;OppPPG &quot;</span>, <span class="st">&quot;Unadjusted Off Efficiency &quot;</span>, <span class="st">&quot;Unadjusted Def Efficiency &quot;</span>, <span class="st">&quot;WL% &quot;</span>, <span class="st">&quot;Pace &quot;</span>, <span class="st">&quot;eFG% &quot;</span>, <span class="st">&quot;OReb &quot;</span>, <span class="st">&quot;AvgMOV &quot;</span>, <span class="st">&quot;Extra Scoring ChancesPG &quot;</span>, <span class="st">&quot;Pythagorean Expectation &quot;</span>, <span class="st">&quot;Luck &quot;</span>, <span class="st">&quot;RPI &quot;</span>, <span class="st">&quot;RPI Strength of Schedule &quot;</span>, <span class="st">&quot;WL% Past 10 &quot;</span>, <span class="st">&quot;Adjusted Off Efficiency &quot;</span>, <span class="st">&quot;Adjusted Def Efficiency &quot;</span>, <span class="st">&quot;Adjusted Efficiency Margin &quot;</span>, <span class="st">&quot;BARTHAG &quot;</span>, <span class="st">&quot;Adjusted Tempo &quot;</span>, <span class="st">&quot;Wins Above Bubble &quot;</span>, <span class="st">&quot;OppTO &quot;</span>, <span class="st">&quot;Adjusted Tempo/Adjusted Efficiency Margin Ratio &quot;</span>, <span class="st">&quot;OppTO/TO Ratio &quot;</span>)</span>
<span id="cb11-6"><a href="index.html#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="index.html#cb11-7" tabindex="-1"></a>corrplot<span class="sc">::</span><span class="fu">corrplot</span>(res,<span class="at">type =</span> <span class="st">&quot;upper&quot;</span>, <span class="at">order =</span> <span class="st">&quot;hclust&quot;</span>, </span>
<span id="cb11-8"><a href="index.html#cb11-8" tabindex="-1"></a>                   <span class="at">tl.col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">tl.srt =</span> <span class="dv">75</span>, <span class="at">tl.cex =</span> <span class="fl">0.6</span>,</span>
<span id="cb11-9"><a href="index.html#cb11-9" tabindex="-1"></a><span class="co">#                   title = &quot;Correlation Matrix on March Madness Matchup Data (2008-2022)&quot;, </span></span>
<span id="cb11-10"><a href="index.html#cb11-10" tabindex="-1"></a>                   <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Next, I also wanted to see these correlations in a tabular format so I could quickly sort through the strongest correlations. I used a function I found online to flatten out the correlation matrix to pair relationships.</p>
<p>There are some strong correlations which will be good to note in terms of multicollinearity as I continue the model building process. For example, Adjusted Tempo and Pace are highly correlated.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="index.html#cb12-1" tabindex="-1"></a>flattenCorrMatrix <span class="ot">&lt;-</span> <span class="cf">function</span>(cormat) {</span>
<span id="cb12-2"><a href="index.html#cb12-2" tabindex="-1"></a>  ut <span class="ot">&lt;-</span> <span class="fu">upper.tri</span>(cormat)</span>
<span id="cb12-3"><a href="index.html#cb12-3" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb12-4"><a href="index.html#cb12-4" tabindex="-1"></a>    <span class="at">row =</span> <span class="fu">rownames</span>(cormat)[<span class="fu">row</span>(cormat)[ut]],</span>
<span id="cb12-5"><a href="index.html#cb12-5" tabindex="-1"></a>    <span class="at">column =</span> <span class="fu">rownames</span>(cormat)[<span class="fu">col</span>(cormat)[ut]],</span>
<span id="cb12-6"><a href="index.html#cb12-6" tabindex="-1"></a>    <span class="at">cor  =</span>(cormat)[ut]</span>
<span id="cb12-7"><a href="index.html#cb12-7" tabindex="-1"></a>  )</span>
<span id="cb12-8"><a href="index.html#cb12-8" tabindex="-1"></a>}</span>
<span id="cb12-9"><a href="index.html#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="index.html#cb12-10" tabindex="-1"></a>prev.corrs <span class="ot">&lt;-</span> train[<span class="fu">c</span>(<span class="dv">11</span><span class="sc">:</span><span class="dv">44</span>)]</span>
<span id="cb12-11"><a href="index.html#cb12-11" tabindex="-1"></a>res2<span class="ot">&lt;-</span><span class="fu">rcorr</span>(<span class="fu">as.matrix</span>(prev.corrs))</span>
<span id="cb12-12"><a href="index.html#cb12-12" tabindex="-1"></a>previous.correlations <span class="ot">&lt;-</span> <span class="fu">flattenCorrMatrix</span>(res2<span class="sc">$</span>r)</span>
<span id="cb12-13"><a href="index.html#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="index.html#cb12-14" tabindex="-1"></a><span class="fu">head</span>(dplyr<span class="sc">::</span><span class="fu">arrange</span>(previous.correlations, <span class="fu">desc</span>(cor)),<span class="dv">20</span>)</span></code></pre></div>
<pre><code>##                     row                  column       cor
## 1       Diff_AvgMOV_PRE Diff_PythagoreanExp_PRE 0.9657030
## 2         Diff_Pace_PRE           Diff_AdjT_PRE 0.9596952
## 3        Diff_AdjEM_PRE        Diff_BARTHAG_PRE 0.9482559
## 4        Diff_FGPct_PRE         Diff_eFGPct_PRE 0.9174768
## 5     Diff_TotalRPI_PRE            Diff_WAB_PRE 0.9161048
## 6          Diff_STL_PRE          Diff_OppTO_PRE 0.8705419
## 7        Diff_AdjEM_PRE            Diff_WAB_PRE 0.8668055
## 8      Diff_BARTHAG_PRE            Diff_WAB_PRE 0.8584455
## 9  Diff_UnadjOffEff_PRE           Diff_AdjO_PRE 0.8573941
## 10 Diff_UnadjDefEff_PRE           Diff_AdjD_PRE 0.8188057
## 11    Diff_TotalRPI_PRE          Diff_AdjEM_PRE 0.8158615
## 12    Diff_TotalRPI_PRE        Diff_BARTHAG_PRE 0.8125139
## 13 Diff_UnadjOffEff_PRE         Diff_eFGPct_PRE 0.8047997
## 14       Diff_WLPct_PRE Diff_PythagoreanExp_PRE 0.7954170
## 15       Diff_WLPct_PRE         Diff_AvgMOV_PRE 0.7945533
## 16        Diff_AdjO_PRE          Diff_AdjEM_PRE 0.7817846
## 17      Diff_OppPPG_PRE           Diff_Pace_PRE 0.7634150
## 18         Diff_PPG_PRE           Diff_AdjT_PRE 0.7581140
## 19    Diff_OppFGPct_PRE    Diff_UnadjDefEff_PRE 0.7497411
## 20      Diff_OppPPG_PRE    Diff_UnadjDefEff_PRE 0.7468812</code></pre>
<p><strong>Visual Exploration</strong></p>
<p>I wanted to start with some quick tournament performance evaluations on tournament performance for the training data of 2008 to 2022. As expected, the higher seed wins any given matchup 70.8% of the time.
<img src="bookdownproj_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>I was also interested in breaking down these wins by round of tournament and team seed. We expect to see the higher seed win more matchups than the lower seed across all rounds of the NCAA tournament. However, it is interesting to note that the Elite 8 has almost 45% of the lower seeds winning tournament games, bringing slightly more parity to this round of the tournament.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="index.html#cb14-1" tabindex="-1"></a><span class="co">#find proportions of wins and losses</span></span>
<span id="cb14-2"><a href="index.html#cb14-2" tabindex="-1"></a>proportions <span class="ot">&lt;-</span> <span class="fu">aggregate</span>(Winner_A <span class="sc">~</span> Round, <span class="at">data =</span> train, <span class="at">FUN =</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="at">Prop_Winner =</span> <span class="fu">mean</span>(x), <span class="at">Prop_Loser =</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(x)))</span>
<span id="cb14-3"><a href="index.html#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="index.html#cb14-4" tabindex="-1"></a><span class="co"># Convert result to data frame</span></span>
<span id="cb14-5"><a href="index.html#cb14-5" tabindex="-1"></a>winners <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(proportions<span class="sc">$</span>Winner_A)</span>
<span id="cb14-6"><a href="index.html#cb14-6" tabindex="-1"></a>proportions <span class="ot">&lt;-</span> <span class="fu">cbind</span>(proportions<span class="sc">$</span>Round, winners)</span>
<span id="cb14-7"><a href="index.html#cb14-7" tabindex="-1"></a><span class="fu">colnames</span>(proportions)[<span class="dv">1</span>] <span class="ot">=</span> <span class="st">&quot;Round&quot;</span></span>
<span id="cb14-8"><a href="index.html#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="index.html#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="index.html#cb14-10" tabindex="-1"></a>melted_data <span class="ot">&lt;-</span> reshape2<span class="sc">::</span><span class="fu">melt</span>(proportions, <span class="at">id.vars =</span> <span class="st">&quot;Round&quot;</span>)</span>
<span id="cb14-11"><a href="index.html#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="index.html#cb14-12" tabindex="-1"></a><span class="co"># Plot side-by-side barplot using facets</span></span>
<span id="cb14-13"><a href="index.html#cb14-13" tabindex="-1"></a><span class="fu">ggplot</span>(melted_data, <span class="fu">aes</span>(<span class="at">x =</span> variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb14-14"><a href="index.html#cb14-14" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb14-15"><a href="index.html#cb14-15" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> Round) <span class="sc">+</span></span>
<span id="cb14-16"><a href="index.html#cb14-16" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">name =</span> <span class="st">&quot;Outcome&quot;</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Prop_Winner&quot;</span> <span class="ot">=</span> <span class="st">&quot;skyblue&quot;</span>, <span class="st">&quot;Prop_Loser&quot;</span> <span class="ot">=</span> <span class="st">&quot;salmon&quot;</span>)) <span class="sc">+</span></span>
<span id="cb14-17"><a href="index.html#cb14-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Proportions of Games Won by the Higher Seed by Round&quot;</span>, <span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="st">&quot;Proportion&quot;</span>) <span class="sc">+</span></span>
<span id="cb14-18"><a href="index.html#cb14-18" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>In regard to seeds, the bar chart breaks down total wins by a higher and lower seed. The sky blue color represents every time a certain seed was higher than its opponents, while the salmon color represents every time a certain seed was lower than its opponents. For example, a 16 seed will always be the lower seed (unless an instance occurred where it was playing another 16 seed in the Final Four and had a higher points per game than its opponent) and we know that a 16 seed has only won one time over the course of the training set (2008 - 2022).</p>
<p>1 seeds are usually the higher seed and clearly dominate in their matchups. There have been certain instances where some 1 seeds have played other 1 seeds, in which case one of them would have to be the “lower seed” for that matchup. One interesting trend in the bar chart below is the 11 and 12 seeds. 11 seeds have the 5th highest number of wins, despite being the underdog in their matchups.
<img src="bookdownproj_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Lastly, I wanted to provide a quick visual of what teams have played in the most March Madness games. Kansas, UNC, and Gonzaga lead the way with the majority of tournament appearances. This does not necessarily mean that they will win matchups, but it does indicate that they are teams who are experienced in showing up at the March Madness tournament.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="index.html#cb15-1" tabindex="-1"></a>allTeams <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">unlist</span>(mm_data<span class="sc">$</span>TeamA),<span class="fu">unlist</span>(mm_data<span class="sc">$</span>TeamB))</span>
<span id="cb15-2"><a href="index.html#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="index.html#cb15-3" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">sort</span>(<span class="fu">table</span>(allTeams), <span class="at">decreasing =</span> <span class="cn">TRUE</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>],</span>
<span id="cb15-4"><a href="index.html#cb15-4" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">&#39;Top 10 Teams to Play in March Madness (2008-2022)&#39;</span>,</span>
<span id="cb15-5"><a href="index.html#cb15-5" tabindex="-1"></a>        <span class="at">xlab =</span> <span class="st">&#39;Teams&#39;</span>,</span>
<span id="cb15-6"><a href="index.html#cb15-6" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">&#39;Total Games Played&#39;</span>, <span class="at">las =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>I did also explore a variable that I later removed from the model. It was a binary flag indicator for whether or not the team was top 12 in the week 6 AP Poll, as the champion has always top 12 in the week 6 AP Poll. While correlation does not equal causation, I thought it would be interesting to include as a variable for analysis.</p>
<p>Using both a Chi-Square test and a Mantel-Haenszel test, I found that the distribution of the higher seed winning a matchup changes across the distribution of a higher seed of losing a matchup. The Mantel-Haenszel test confirmed a linear association between the two distributions.</p>
<p>Using odds ratios, we can interpret this as teams in the top 12 week 6 AP Poll are 1.64 times more likely to win their matchup than lose for teams not in the top 12 week 6 AP Poll.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="index.html#cb16-1" tabindex="-1"></a><span class="fu">CrossTable</span>(train<span class="sc">$</span>Winner_A, train<span class="sc">$</span>Top12_Week6APPoll, <span class="at">expected =</span> <span class="cn">TRUE</span>) <span class="co">#assumptions passed - expected values greater than 5</span></span>
<span id="cb16-2"><a href="index.html#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="index.html#cb16-3" tabindex="-1"></a><span class="fu">chisq.test</span>(<span class="fu">table</span>(train<span class="sc">$</span>Winner_A, train<span class="sc">$</span>Top12_Week6APPoll)) <span class="co">#significant, &lt; 2.67e-05... so the distribution of one variable changes across the distribution of another variable</span></span>
<span id="cb16-4"><a href="index.html#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="index.html#cb16-5" tabindex="-1"></a><span class="co">#mantel haenszel test</span></span>
<span id="cb16-6"><a href="index.html#cb16-6" tabindex="-1"></a><span class="fu">CMHtest</span>(<span class="fu">table</span>(train<span class="sc">$</span>Winner_A, train<span class="sc">$</span>Top12_Week6APPoll))<span class="sc">$</span>table[<span class="dv">1</span>,] <span class="co">#linear association, &lt; 2.05e-05</span></span>
<span id="cb16-7"><a href="index.html#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="index.html#cb16-8" tabindex="-1"></a><span class="co">#interpretation</span></span>
<span id="cb16-9"><a href="index.html#cb16-9" tabindex="-1"></a><span class="fu">OddsRatio</span>(<span class="fu">table</span>(train<span class="sc">$</span>Winner_A, train<span class="sc">$</span>Top12_Week6APPoll)) <span class="co">#1.637964 for train</span></span></code></pre></div>
</div>
<div id="modeling" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Modeling<a href="index.html#modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before starting modeling, I explored creating some additional variables to improve predictability in the model. The first variable I created was the Pythagorean Expectation of a team’s winning percentage. It is the expected winning percentage a team should have had, given the number of points they scored and allowed in a season. I provide code in the appendix on how I optimized the exponent for this formula.</p>
<p>Additionally, I created another variable that describes a team’s luck, which is the difference between a team’s expected win percentage (Pythagorean Expectation) and their actual win percentage. I figured if a team’s expected win percentage of 0.5, but they are actually at 0.625, then maybe they are a lucky team and winning some close games.</p>
<p>Lastly, I explored two additional variables: Adjusted Tempo / Adjusted Efficiency Margin and the Opponent Turnovers / Team Turnover ratios. These have been useful in flagging certain strengths in teams that can possibly generate upsets for the March Madness tournament.</p>
<p><strong>Conservative Alpha Level and Weighting</strong></p>
<p>A conservative alpha level of 0.01, instead of 0.05, was chosen for model significance testing since there are 881 observations in the training set and we need to adjust it to reflect a higher sample size.</p>
<p>Before modeling, I did also explore creating weights, so the model puts higher importance on getting Round of 64 matchups right versus a Championship matchup correct. I used a weighting scheme on how ESPN calculates March Madness points. I explored this possibility because my predictions for the Elite 8 won’t matter if I don’t have any correct predictions of the teams in the Elite 8.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="index.html#cb17-1" tabindex="-1"></a>train <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="index.html#cb17-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">wts =</span> <span class="fu">case_when</span>(</span>
<span id="cb17-3"><a href="index.html#cb17-3" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Round64&#39;</span> <span class="sc">~</span> <span class="dv">320</span>,</span>
<span id="cb17-4"><a href="index.html#cb17-4" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Round32&#39;</span> <span class="sc">~</span> <span class="dv">160</span>,</span>
<span id="cb17-5"><a href="index.html#cb17-5" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Sweet16&#39;</span> <span class="sc">~</span> <span class="dv">80</span>,</span>
<span id="cb17-6"><a href="index.html#cb17-6" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Elite8&#39;</span> <span class="sc">~</span> <span class="dv">40</span>,</span>
<span id="cb17-7"><a href="index.html#cb17-7" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Final4&#39;</span> <span class="sc">~</span> <span class="dv">20</span>,</span>
<span id="cb17-8"><a href="index.html#cb17-8" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Championship&#39;</span> <span class="sc">~</span> <span class="dv">10</span>,</span>
<span id="cb17-9"><a href="index.html#cb17-9" tabindex="-1"></a>    <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>  <span class="co"># Default condition, if none of the above conditions are met</span></span>
<span id="cb17-10"><a href="index.html#cb17-10" tabindex="-1"></a>  ))</span>
<span id="cb17-11"><a href="index.html#cb17-11" tabindex="-1"></a></span>
<span id="cb17-12"><a href="index.html#cb17-12" tabindex="-1"></a>valid <span class="ot">&lt;-</span> valid <span class="sc">%&gt;%</span></span>
<span id="cb17-13"><a href="index.html#cb17-13" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">wts =</span> <span class="fu">case_when</span>(</span>
<span id="cb17-14"><a href="index.html#cb17-14" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Round64&#39;</span> <span class="sc">~</span> <span class="dv">320</span>,</span>
<span id="cb17-15"><a href="index.html#cb17-15" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Round32&#39;</span> <span class="sc">~</span> <span class="dv">160</span>,</span>
<span id="cb17-16"><a href="index.html#cb17-16" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Sweet16&#39;</span> <span class="sc">~</span> <span class="dv">80</span>,</span>
<span id="cb17-17"><a href="index.html#cb17-17" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Elite8&#39;</span> <span class="sc">~</span> <span class="dv">40</span>,</span>
<span id="cb17-18"><a href="index.html#cb17-18" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Final4&#39;</span> <span class="sc">~</span> <span class="dv">20</span>,</span>
<span id="cb17-19"><a href="index.html#cb17-19" tabindex="-1"></a>    Round <span class="sc">==</span> <span class="st">&#39;Championship&#39;</span> <span class="sc">~</span> <span class="dv">10</span>,</span>
<span id="cb17-20"><a href="index.html#cb17-20" tabindex="-1"></a>    <span class="cn">TRUE</span> <span class="sc">~</span> <span class="dv">0</span>  <span class="co"># Default condition, if none of the above conditions are met</span></span>
<span id="cb17-21"><a href="index.html#cb17-21" tabindex="-1"></a>  ))</span></code></pre></div>
<p><strong>Logistic Regression</strong></p>
<p>Logistic Regression is by far the most time intensive model in terms of validating assumptions, binning, and coding out the model. I start by validating assumptions of the logistic regression model in my dataset, which are independent rows and the continuous predictor variables are linearly related to the logit.</p>
<p>I first explored a distribution of each variable in the model (output can be found in the appendix). Most variables appeared to be relatively normal, with only slight skewness and not needing any major transformations.</p>
<p>In regard to checking the linearity assumption for the continuous predictor variables, I fit a GAM model and a logistic regression model with only one of the predictor variables at a time. Next, I run an Chi-Square ANOVA test to see if there is a statistically significant difference between the GAM model and the logit model. The loop prints out for each variable if the assumption was met or not met.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="index.html#cb18-1" tabindex="-1"></a>var.names <span class="ot">&lt;-</span> <span class="fu">colnames</span>(train)[<span class="fu">c</span>(<span class="dv">11</span><span class="sc">:</span><span class="dv">44</span>)]</span>
<span id="cb18-2"><a href="index.html#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="index.html#cb18-3" tabindex="-1"></a>assumption_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;Variable&quot;</span> <span class="ot">=</span> <span class="fu">as.character</span>(), <span class="st">&quot;Assumption&quot;</span> <span class="ot">=</span> <span class="fu">as.character</span>(), <span class="st">&quot;P-Value&quot;</span> <span class="ot">=</span> <span class="fu">as.numeric</span>(), <span class="st">&quot;EDF&quot;</span> <span class="ot">=</span> <span class="fu">as.numeric</span>())</span>
<span id="cb18-4"><a href="index.html#cb18-4" tabindex="-1"></a></span>
<span id="cb18-5"><a href="index.html#cb18-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> var.names) {</span>
<span id="cb18-6"><a href="index.html#cb18-6" tabindex="-1"></a>  xvar <span class="ot">&lt;-</span> (train[[i]]) <span class="co">#this is needed to get the gam to run</span></span>
<span id="cb18-7"><a href="index.html#cb18-7" tabindex="-1"></a>  fit.gam <span class="ot">&lt;-</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(Winner_A<span class="sc">~</span><span class="fu">s</span>(xvar), <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="at">method =</span> <span class="st">&quot;REML&quot;</span>)</span>
<span id="cb18-8"><a href="index.html#cb18-8" tabindex="-1"></a>  logit.model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Winner_A <span class="sc">~</span> train[[i]], <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb18-9"><a href="index.html#cb18-9" tabindex="-1"></a>  pval <span class="ot">&lt;-</span> <span class="fu">anova</span>(logit.model, fit.gam, <span class="at">test =</span> <span class="st">&#39;Chisq&#39;</span>)<span class="sc">$</span><span class="st">`</span><span class="at">Pr(&gt;Chi)</span><span class="st">`</span>[<span class="dv">2</span>]</span>
<span id="cb18-10"><a href="index.html#cb18-10" tabindex="-1"></a>  </span>
<span id="cb18-11"><a href="index.html#cb18-11" tabindex="-1"></a>  <span class="cf">if</span> (pval <span class="sc">&lt;</span> <span class="fl">0.01</span>) {</span>
<span id="cb18-12"><a href="index.html#cb18-12" tabindex="-1"></a>    assumption_df[<span class="fu">nrow</span>(assumption_df) <span class="sc">+</span> <span class="dv">1</span>,] <span class="ot">=</span> <span class="fu">c</span>(i, <span class="st">&quot;ASSUMPTION NOT MET&quot;</span>, <span class="fu">round</span>(pval,<span class="dv">5</span>), <span class="fu">summary</span>(fit.gam)<span class="sc">$</span>edf)</span>
<span id="cb18-13"><a href="index.html#cb18-13" tabindex="-1"></a>    <span class="co">#print(paste(i, &quot;**___ASSUMPTION NOT MET___**&quot;, &quot;P-Value - &quot;,round(pval,5), &quot;EDF - &quot;, round(summary(fit.gam)$edf),5))</span></span>
<span id="cb18-14"><a href="index.html#cb18-14" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb18-15"><a href="index.html#cb18-15" tabindex="-1"></a>    assumption_df[<span class="fu">nrow</span>(assumption_df) <span class="sc">+</span> <span class="dv">1</span>,] <span class="ot">=</span> <span class="fu">c</span>(i, <span class="st">&quot;Passed&quot;</span>, <span class="fu">round</span>(pval,<span class="dv">5</span>), <span class="fu">summary</span>(fit.gam)<span class="sc">$</span>edf)</span>
<span id="cb18-16"><a href="index.html#cb18-16" tabindex="-1"></a>    <span class="co">#print(paste(i, &quot;Assumption Met&quot;, &quot;P-Value - &quot;, round(pval,5), &quot;EDF - &quot;, round(summary(fit.gam)$edf),5))</span></span>
<span id="cb18-17"><a href="index.html#cb18-17" tabindex="-1"></a>  }</span>
<span id="cb18-18"><a href="index.html#cb18-18" tabindex="-1"></a>  </span>
<span id="cb18-19"><a href="index.html#cb18-19" tabindex="-1"></a>  <span class="co">#plot(fit.gam, xlab = i) Not printing to save space</span></span>
<span id="cb18-20"><a href="index.html#cb18-20" tabindex="-1"></a>}</span>
<span id="cb18-21"><a href="index.html#cb18-21" tabindex="-1"></a>assumption_df</span></code></pre></div>
<pre><code>##                          Variable         Assumption P.Value              EDF
## 1                  Diff_FGPct_PRE ASSUMPTION NOT MET 0.00295 1.00085223123293
## 2                 Diff_3ptPct_PRE             Passed 0.02428  1.0125619267051
## 3                  Diff_FTPct_PRE ASSUMPTION NOT MET 0.00209 1.00050515158461
## 4                    Diff_TRB_PRE             Passed 0.01581 4.03744064066963
## 5                    Diff_AST_PRE ASSUMPTION NOT MET 0.00184 1.00045544311534
## 6                    Diff_STL_PRE ASSUMPTION NOT MET 0.00194  1.0004875277415
## 7                    Diff_BLK_PRE ASSUMPTION NOT MET  0.0046 1.00142575210348
## 8                     Diff_TO_PRE ASSUMPTION NOT MET 0.00443 1.00141951965646
## 9                     Diff_PF_PRE ASSUMPTION NOT MET 0.00341 1.00090274773104
## 10                   Diff_PPG_PRE             Passed 0.01441 2.46334607317298
## 11              Diff_OppFGPct_PRE ASSUMPTION NOT MET 0.00194 1.00038822899549
## 12                Diff_OppPPG_PRE             Passed 0.13386 1.39877172626676
## 13           Diff_UnadjOffEff_PRE             Passed 0.04258 2.32023437242797
## 14           Diff_UnadjDefEff_PRE             Passed 0.11128 1.75942492354625
## 15                 Diff_WLPct_PRE ASSUMPTION NOT MET 0.00037 2.97107705766867
## 16                  Diff_Pace_PRE ASSUMPTION NOT MET 0.00174 1.00040602998296
## 17                Diff_eFGPct_PRE             Passed 0.07707 2.01553457925158
## 18                  Diff_OReb_PRE ASSUMPTION NOT MET 0.00261 1.00072109420806
## 19                Diff_AvgMOV_PRE             Passed  0.0284 2.36311977046627
## 20 Diff_ExtraScoringChancesPG_PRE             Passed 0.02128 3.38947676743612
## 21        Diff_PythagoreanExp_PRE             Passed 0.04199 2.42170439852336
## 22                  Diff_Luck_PRE ASSUMPTION NOT MET 0.00232 1.00057799563743
## 23              Diff_TotalRPI_PRE ASSUMPTION NOT MET 0.00437 2.75445353243539
## 24               Diff_RPI_SOS_PRE ASSUMPTION NOT MET 0.00927 2.60679450082052
## 25              Diff_WLPast10_PRE             Passed  0.1254 1.60247272897589
## 26                  Diff_AdjO_PRE ASSUMPTION NOT MET 0.00615 1.00216029670938
## 27                  Diff_AdjD_PRE ASSUMPTION NOT MET 0.00683 2.82051402221713
## 28                 Diff_AdjEM_PRE ASSUMPTION NOT MET 0.00187 1.00047561758484
## 29               Diff_BARTHAG_PRE             Passed 0.07643 2.45765558669723
## 30                  Diff_AdjT_PRE ASSUMPTION NOT MET 0.00355 1.00096187219166
## 31                   Diff_WAB_PRE ASSUMPTION NOT MET 0.00254 1.00057395976496
## 32                 Diff_OppTO_PRE ASSUMPTION NOT MET 0.00184 1.00036884429105
## 33      Diff_AdjT_AdjEM_Ratio_PRE ASSUMPTION NOT MET       0 4.47781005966761
## 34    Diff_OppTO_TeamTO_Ratio_PRE ASSUMPTION NOT MET  0.0014 1.00031780616377</code></pre>
<p>Since only 12 of the 34 variables passed the linearity assumption, I decided to explore strategically binning all continuous variables via SMBinning. The code below finds the optimal splits in a continuous variable that still capture the signal associated with predicting the winner of the matchup. An example of the distribution of the Pythagorean Expectation and average margin of victory (MOV) variables are provided below. As you can see, there is a fairly stark difference in the average MOV buckets, which should make for a good variable in the model.</p>
<p>The third plot shows the strongest predictors within the model based on information value (IV). IV measures the ability of a variable to separate winners and losers. While this metric is mostly used for variable selection in banking, I find it effective in this situation as well. I think it’s important to recognize that a common cutoff in banking is an IV greater than or equal to 0.1 to select variables, however, I wanted to keep all binned variables that were binned with an IV greater than or equal to 0.01 (which ended up being all variables binned).</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-19-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<pre><code>##  
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |-                                                 |   3%
  |                                                        
  |---                                               |   6%
  |                                                        
  |----                                              |   8%
  |                                                        
  |------                                            |  11%
  |                                                        
  |-------                                           |  14%
  |                                                        
  |--------                                          |  17%
  |                                                        
  |----------                                        |  19%
  |                                                        
  |-----------                                       |  22%
  |                                                        
  |------------                                      |  25%
  |                                                        
  |--------------                                    |  28%
  |                                                        
  |---------------                                   |  31%
  |                                                        
  |-----------------                                 |  33%
  |                                                        
  |------------------                                |  36%
  |                                                        
  |-------------------                               |  39%
  |                                                        
  |---------------------                             |  42%
  |                                                        
  |----------------------                            |  44%
  |                                                        
  |------------------------                          |  47%
  |                                                        
  |-------------------------                         |  50%
  |                                                        
  |--------------------------                        |  53%
  |                                                        
  |----------------------------                      |  56%
  |                                                        
  |-----------------------------                     |  58%
  |                                                        
  |-------------------------------                   |  61%
  |                                                        
  |--------------------------------                  |  64%
  |                                                        
  |---------------------------------                 |  67%
  |                                                        
  |-----------------------------------               |  69%
  |                                                        
  |------------------------------------              |  72%
  |                                                        
  |--------------------------------------            |  75%
  |                                                        
  |---------------------------------------           |  78%
  |                                                        
  |----------------------------------------          |  81%
  |                                                        
  |------------------------------------------        |  83%
  |                                                        
  |-------------------------------------------       |  86%
  |                                                        
  |--------------------------------------------      |  89%
  |                                                        
  |----------------------------------------------    |  92%
  |                                                        
  |-----------------------------------------------   |  94%
  |                                                        
  |------------------------------------------------- |  97%
  |                                                        
  |--------------------------------------------------| 100%
## </code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-19-3.png" width="672" /></p>
<p>The NCAA tournament selection committee, up until 2018, used the ratings percentage index (RPI), to decide which teams should play in the March Madness tournament or not. After 2018 they decided to switch to NCAA evaluation tool (NET) to pick teams, as it does a better job at evaluating a team’s strength of schedule. Since the committee no longer uses this RPI variable to select teams, I was unsure of whether or not to include this variable in the model.</p>
<p>My solution was to perform a Likelihood Ratio Rest for a model with and without the two RPI variables (total team RPI and team RPI strength of schedule). The results of the test fails to reject the null hypothesis, proving that the smaller model (without RPI variables) is the better model. I used AIC and BIC to prove this evaluation and did decide to drop the RPI variables.</p>
<pre><code>## Likelihood ratio test
## 
## Model 1: Winner_A ~ Diff_FGPct_PRE_bin + Diff_AST_PRE_bin + Diff_STL_PRE_bin + 
##     Diff_BLK_PRE_bin + Diff_TO_PRE_bin + Diff_PF_PRE_bin + Diff_PPG_PRE_bin + 
##     Diff_OppPPG_PRE_bin + Diff_UnadjOffEff_PRE_bin + Diff_UnadjDefEff_PRE_bin + 
##     Diff_WLPct_PRE_bin + Diff_OReb_PRE_bin + Diff_AvgMOV_PRE_bin + 
##     Diff_ExtraScoringChancesPG_PRE_bin + Diff_PythagoreanExp_PRE_bin + 
##     Diff_Luck_PRE_bin + Diff_TotalRPI_PRE_bin + Diff_RPI_SOS_PRE_bin + 
##     Diff_AdjO_PRE_bin + Diff_AdjD_PRE_bin + Diff_AdjEM_PRE_bin + 
##     Diff_BARTHAG_PRE_bin + Diff_WAB_PRE_bin + Diff_OppTO_PRE_bin + 
##     Diff_OppTO_TeamTO_Ratio_PRE_bin
## Model 2: Winner_A ~ Diff_FGPct_PRE_bin + Diff_AST_PRE_bin + Diff_STL_PRE_bin + 
##     Diff_BLK_PRE_bin + Diff_TO_PRE_bin + Diff_PF_PRE_bin + Diff_PPG_PRE_bin + 
##     Diff_OppPPG_PRE_bin + Diff_UnadjOffEff_PRE_bin + Diff_UnadjDefEff_PRE_bin + 
##     Diff_WLPct_PRE_bin + Diff_OReb_PRE_bin + Diff_AvgMOV_PRE_bin + 
##     Diff_ExtraScoringChancesPG_PRE_bin + Diff_PythagoreanExp_PRE_bin + 
##     Diff_Luck_PRE_bin + Diff_AdjO_PRE_bin + Diff_AdjD_PRE_bin + 
##     Diff_AdjEM_PRE_bin + Diff_BARTHAG_PRE_bin + Diff_WAB_PRE_bin + 
##     Diff_OppTO_PRE_bin + Diff_OppTO_TeamTO_Ratio_PRE_bin
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)
## 1  41 -423.02                     
## 2  37 -424.46 -4 2.8677     0.5802</code></pre>
<pre><code>## [1] &quot;Model with RPI variables -  (AIC) 928.045 (BIC) 1124.068&quot;</code></pre>
<pre><code>## [1] &quot;Model without RPI variables -  (AIC) 922.912 (BIC) 1099.811&quot;</code></pre>
<p>Now, let’s jump into the logistic regression model. By converting all variables to categorical via SMBinning, the logistic regression has now passed all of its assumptions. I added many variables after strategically binning, which all had an IV of at least 0.01. I decided to perform backward selection first using a significance level of 0.01. This left me with 3 binned variables: Difference in Steals, Difference in Turnovers, and Difference in Adjusted Efficiency Margin.</p>
<p>I did one final check of multicollinearity with VIF and a likelihood ratio test to ensure all variables were significant and not correlated. Both checks passed.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="index.html#cb24-1" tabindex="-1"></a>new.model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Winner_A <span class="sc">~</span> Diff_STL_PRE_bin <span class="sc">+</span> Diff_TO_PRE_bin <span class="sc">+</span> Diff_AdjEM_PRE_bin, <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb24-2"><a href="index.html#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="index.html#cb24-3" tabindex="-1"></a><span class="co">#Checking multicollinearity and using Likelihood Ratio Test (LRT)</span></span>
<span id="cb24-4"><a href="index.html#cb24-4" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(new.model) <span class="co">#all &lt; 5</span></span></code></pre></div>
<pre><code>##                        GVIF Df GVIF^(1/(2*Df))
## Diff_STL_PRE_bin   1.033577  1        1.016650
## Diff_TO_PRE_bin    1.033610  1        1.016666
## Diff_AdjEM_PRE_bin 1.015016  3        1.002487</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="index.html#cb26-1" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(new.model, <span class="at">test =</span> <span class="st">&#39;LR&#39;</span>, <span class="at">type =</span> <span class="st">&#39;III&#39;</span>) <span class="co">#all sig</span></span></code></pre></div>
<pre><code>## Analysis of Deviance Table (Type III tests)
## 
## Response: Winner_A
##                    LR Chisq Df            Pr(&gt;Chisq)    
## Diff_STL_PRE_bin     21.224  1           0.000004086 ***
## Diff_TO_PRE_bin       9.296  1              0.002296 ** 
## Diff_AdjEM_PRE_bin  112.847  3 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>After building the model, I did a quick evaluation metric using the coefficient of discrimination (plotting how well the model separates winners and losers). It appears the model does a pretty good job of discriminating between winners and losers when the predicted probability is either very high or very low. However, predicted probabilities closer to 0.45 to 0.75 are a complete tossup and it does quite poorly.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p><strong>Random Forest</strong></p>
<p>The next model I tried was a Random Forest model. Firstly, I want to see what the best number of trees is to use for the model. It appears the MSE levels out around 250 trees, but to be safe since computing time isn’t an issue here, I use 500 trees.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>The next step is to tune the Random Forest model with 500 trees. I need to figure out what the optimal number of random variables to use at each split is, so I create an elbow plot. The out-of-bag observation error is minimized with an mtry of 5, so that is what I will use to train the model.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="index.html#cb28-1" tabindex="-1"></a><span class="co">#tuning model</span></span>
<span id="cb28-2"><a href="index.html#cb28-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb28-3"><a href="index.html#cb28-3" tabindex="-1"></a><span class="fu">tuneRF</span>(<span class="at">x =</span> train[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">y =</span> train[,<span class="dv">1</span>], <span class="co">#column 1 has response variable</span></span>
<span id="cb28-4"><a href="index.html#cb28-4" tabindex="-1"></a>       <span class="at">plot =</span> <span class="cn">TRUE</span>, <span class="at">ntreeTry =</span> <span class="dv">500</span>, <span class="at">stepFactor =</span> <span class="fl">0.5</span>, <span class="at">method =</span> <span class="st">&#39;class&#39;</span>)</span></code></pre></div>
<pre><code>## mtry = 5  OOB error = 27.81% 
## Searching left ...
## mtry = 10    OOB error = 28.72% 
## -0.03265306 0.05 
## Searching right ...
## mtry = 2     OOB error = 28.15% 
## -0.0122449 0.05</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre><code>##        mtry  OOBError
## 2.OOB     2 0.2814983
## 5.OOB     5 0.2780931
## 10.OOB   10 0.2871737</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="index.html#cb31-1" tabindex="-1"></a><span class="co">#above output said optimal mtry is 5.</span></span>
<span id="cb31-2"><a href="index.html#cb31-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb31-3"><a href="index.html#cb31-3" tabindex="-1"></a>rf.mm <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Winner_A <span class="sc">~</span>., </span>
<span id="cb31-4"><a href="index.html#cb31-4" tabindex="-1"></a>                      <span class="at">data =</span> train, <span class="at">ntree =</span> <span class="dv">500</span>, <span class="at">mtry =</span> <span class="dv">5</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>,</span>
<span id="cb31-5"><a href="index.html#cb31-5" tabindex="-1"></a>                      <span class="at">method =</span> <span class="st">&#39;class&#39;</span>)</span></code></pre></div>
<p>Lastly, I thought it might be interesting to include a variable importance plot. Using mean decrease in accuracy as well as mean decrease in Gini as evaluation metrics, it appears that the difference in two teams’ adjusted efficiency margin, wins above bubble, BARTHAG, adjusted offensive + defensive efficiency, and the adjusted tempo/adjusted efficiency margin ratio are all top predictors in the model.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p><strong>XGBoost</strong></p>
<p>The next model I tried was an Xtreme Gradient Boosting (XGBoost) model. Like a tree-based algorithm that Random Forest uses, this model sequentially builds its trees based on the error of the previous split.</p>
<p>I start by running a cross validation search to minimize the number of trees on the test root mean square error (RMSE). We see that 3 trees appears to be optimal.</p>
<p>I then run a grid search across number of trees, learning rate, maximum depth of the trees, and subsampling. XGBoost is very flexible for hyperparameter tuning and grid searching. Lastly, I run these possible grid values through a 10-fold cross validation with XGBoost to determine the optimal parameters.</p>
<pre><code>## [1]  train-rmse:0.430903+0.003096    test-rmse:0.468932+0.013417 
## [2]  train-rmse:0.384286+0.005898    test-rmse:0.453792+0.020092 
## [3]  train-rmse:0.345381+0.005273    test-rmse:0.445784+0.021019 
## [4]  train-rmse:0.316384+0.006835    test-rmse:0.446950+0.022418 
## [5]  train-rmse:0.293757+0.009228    test-rmse:0.446562+0.025600 
## [6]  train-rmse:0.273030+0.009880    test-rmse:0.451337+0.027964 
## [7]  train-rmse:0.256176+0.007530    test-rmse:0.452004+0.025227 
## [8]  train-rmse:0.241149+0.008816    test-rmse:0.455333+0.024287 
## [9]  train-rmse:0.228281+0.010449    test-rmse:0.456565+0.024094 
## [10] train-rmse:0.216687+0.010180    test-rmse:0.457721+0.026740 
## [11] train-rmse:0.202741+0.010538    test-rmse:0.459428+0.028016 
## [12] train-rmse:0.194377+0.010288    test-rmse:0.461361+0.029511 
## [13] train-rmse:0.181640+0.009770    test-rmse:0.463641+0.029259 
## [14] train-rmse:0.171866+0.012194    test-rmse:0.464514+0.030984 
## [15] train-rmse:0.163150+0.012990    test-rmse:0.464894+0.032071 
## [16] train-rmse:0.154603+0.011765    test-rmse:0.464739+0.031388 
## [17] train-rmse:0.146318+0.011443    test-rmse:0.465276+0.031111 
## [18] train-rmse:0.138805+0.011136    test-rmse:0.465308+0.031239 
## [19] train-rmse:0.129978+0.011959    test-rmse:0.465801+0.030920 
## [20] train-rmse:0.124520+0.010178    test-rmse:0.465932+0.031246 
## [21] train-rmse:0.117952+0.009996    test-rmse:0.466556+0.031030 
## [22] train-rmse:0.111825+0.010917    test-rmse:0.466651+0.031172 
## [23] train-rmse:0.107445+0.010705    test-rmse:0.466384+0.031661 
## [24] train-rmse:0.103064+0.010074    test-rmse:0.466364+0.031823 
## [25] train-rmse:0.099581+0.009581    test-rmse:0.467014+0.032270 
## [26] train-rmse:0.095859+0.009939    test-rmse:0.467344+0.032388 
## [27] train-rmse:0.090616+0.009554    test-rmse:0.467985+0.032904 
## [28] train-rmse:0.087065+0.009176    test-rmse:0.468165+0.032752 
## [29] train-rmse:0.083210+0.007993    test-rmse:0.468050+0.033065 
## [30] train-rmse:0.079087+0.007444    test-rmse:0.468581+0.032876 
## [31] train-rmse:0.074959+0.007446    test-rmse:0.469065+0.032656 
## [32] train-rmse:0.071553+0.007557    test-rmse:0.468574+0.032335 
## [33] train-rmse:0.067620+0.006159    test-rmse:0.469488+0.031538 
## [34] train-rmse:0.064365+0.006136    test-rmse:0.469474+0.031288 
## [35] train-rmse:0.061643+0.006000    test-rmse:0.469313+0.030902 
## [36] train-rmse:0.058797+0.006732    test-rmse:0.469471+0.030762 
## [37] train-rmse:0.055419+0.006812    test-rmse:0.469968+0.030828 
## [38] train-rmse:0.053188+0.007051    test-rmse:0.470002+0.030991 
## [39] train-rmse:0.050585+0.006540    test-rmse:0.470589+0.031112 
## [40] train-rmse:0.047737+0.006093    test-rmse:0.470679+0.031326 
## [41] train-rmse:0.045689+0.006167    test-rmse:0.470848+0.031427 
## [42] train-rmse:0.043467+0.006012    test-rmse:0.471079+0.031433 
## [43] train-rmse:0.041277+0.005449    test-rmse:0.471322+0.031613 
## [44] train-rmse:0.038859+0.004822    test-rmse:0.471307+0.031817 
## [45] train-rmse:0.037077+0.005037    test-rmse:0.471415+0.031865 
## [46] train-rmse:0.035567+0.005166    test-rmse:0.471054+0.032133 
## [47] train-rmse:0.033748+0.005247    test-rmse:0.471261+0.032097 
## [48] train-rmse:0.032073+0.005383    test-rmse:0.471253+0.032208 
## [49] train-rmse:0.030420+0.004560    test-rmse:0.471464+0.032281 
## [50] train-rmse:0.029104+0.004334    test-rmse:0.471418+0.032343</code></pre>
<p>We see that the best tune to this grid search is 3 trees, a max depth of 2, a learning rate of 0.4, subsample ratio of columns when constructing each tree of 0.9, and subsample ratio of 0.8.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="index.html#cb33-1" tabindex="-1"></a><span class="do">############# grid search to optimize parameters</span></span>
<span id="cb33-2"><a href="index.html#cb33-2" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb33-3"><a href="index.html#cb33-3" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">3</span>, <span class="co">#c(4:6),</span></span>
<span id="cb33-4"><a href="index.html#cb33-4" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>,<span class="fl">0.4</span>, <span class="fl">0.45</span>),</span>
<span id="cb33-5"><a href="index.html#cb33-5" tabindex="-1"></a>  <span class="at">max_depth =</span> <span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>),</span>
<span id="cb33-6"><a href="index.html#cb33-6" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">c</span>(<span class="dv">0</span>),</span>
<span id="cb33-7"><a href="index.html#cb33-7" tabindex="-1"></a>  <span class="at">colsample_bytree =</span> <span class="fu">c</span>(<span class="fl">0.9</span>, <span class="dv">1</span>),</span>
<span id="cb33-8"><a href="index.html#cb33-8" tabindex="-1"></a>  <span class="at">min_child_weight =</span> <span class="dv">1</span>,</span>
<span id="cb33-9"><a href="index.html#cb33-9" tabindex="-1"></a>  <span class="at">subsample =</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="dv">1</span>)</span>
<span id="cb33-10"><a href="index.html#cb33-10" tabindex="-1"></a>)</span>
<span id="cb33-11"><a href="index.html#cb33-11" tabindex="-1"></a></span>
<span id="cb33-12"><a href="index.html#cb33-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>) <span class="co">#</span></span>
<span id="cb33-13"><a href="index.html#cb33-13" tabindex="-1"></a>xgb.ins.caret <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(<span class="at">x =</span> train_x, <span class="at">y =</span> train_y,</span>
<span id="cb33-14"><a href="index.html#cb33-14" tabindex="-1"></a>                              <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb33-15"><a href="index.html#cb33-15" tabindex="-1"></a>                              <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb33-16"><a href="index.html#cb33-16" tabindex="-1"></a>                              <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="co"># Using 10-fold cross-validation</span></span>
<span id="cb33-17"><a href="index.html#cb33-17" tabindex="-1"></a>                                                       <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb33-18"><a href="index.html#cb33-18" tabindex="-1"></a>                              <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb33-19"><a href="index.html#cb33-19" tabindex="-1"></a>                              <span class="at">verbosity =</span><span class="dv">0</span>) <span class="co">#gets rid of warning messages</span></span>
<span id="cb33-20"><a href="index.html#cb33-20" tabindex="-1"></a></span>
<span id="cb33-21"><a href="index.html#cb33-21" tabindex="-1"></a><span class="fu">plot</span>(xgb.ins.caret)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="index.html#cb34-1" tabindex="-1"></a>xgb.ins.caret<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 73       3         2 0.4     0              0.9                1       0.8</code></pre>
<p>Lastly, I define the final model with the tuned parameters and look at variable importance. This plot will automatically cluster variables statistically based on similar gain. We see the most important cluster is the difference in a team’s adjusted efficiency margin. The second most important cluster consists of multiple variables, such as the difference in a team’s wins above bubble, BARTHAG, Opponent Turnover to Team Turnover ratio, Adjusted Offensive Efficiency, Adjusted Tempo to Adjusted Efficiency Margin ratio, Adjusted Defensive Efficiency, and Steals.</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p><strong>Support Vector Machine (SVM)</strong></p>
<p>For the next model to try, I explored a support vector machine (SVM) model. I used the svm() function since this is a classification problem and a radial kernel to capture more non-linearities in the data.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="index.html#cb36-1" tabindex="-1"></a><span class="co">#Building the svm model</span></span>
<span id="cb36-2"><a href="index.html#cb36-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb36-3"><a href="index.html#cb36-3" tabindex="-1"></a>svm.mm <span class="ot">&lt;-</span> <span class="fu">svm</span>(train_x, train_y, <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="at">probability =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>Naive Bayes </strong></p>
<p>Now I explored a Naive Bayes model, another great supervised classification model. This looks to classify based on observations similar characteristics or features and also evaluates based on past decisions of observations similar to this one. This approach is an alternative to the frequentist approach in statistics, with instead trying a Bayesian approach.</p>
<p>I do try a grid search for a few different hyperparameters to try and fit the best Naive Bayes model.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="index.html#cb37-1" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb37-2"><a href="index.html#cb37-2" tabindex="-1"></a>  <span class="at">usekernel =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb37-3"><a href="index.html#cb37-3" tabindex="-1"></a>  <span class="at">fL =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>),</span>
<span id="cb37-4"><a href="index.html#cb37-4" tabindex="-1"></a>  <span class="at">adjust =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)</span>
<span id="cb37-5"><a href="index.html#cb37-5" tabindex="-1"></a>)</span>
<span id="cb37-6"><a href="index.html#cb37-6" tabindex="-1"></a></span>
<span id="cb37-7"><a href="index.html#cb37-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb37-8"><a href="index.html#cb37-8" tabindex="-1"></a>nb.mm.caret <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(Winner_A <span class="sc">~</span> ., <span class="at">data =</span> train,</span>
<span id="cb37-9"><a href="index.html#cb37-9" tabindex="-1"></a>                            <span class="at">method =</span> <span class="st">&quot;nb&quot;</span>, </span>
<span id="cb37-10"><a href="index.html#cb37-10" tabindex="-1"></a>                            <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb37-11"><a href="index.html#cb37-11" tabindex="-1"></a>                            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>))</span>
<span id="cb37-12"><a href="index.html#cb37-12" tabindex="-1"></a></span>
<span id="cb37-13"><a href="index.html#cb37-13" tabindex="-1"></a>nb.mm.caret<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    fL usekernel adjust
## 10  0      TRUE    0.1</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="index.html#cb39-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb39-2"><a href="index.html#cb39-2" tabindex="-1"></a>nb.mm <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(Winner_A <span class="sc">~</span>., <span class="at">data =</span> train, <span class="at">laplace =</span> <span class="dv">0</span>, <span class="at">usekernel =</span> <span class="cn">TRUE</span>, <span class="at">fL =</span> <span class="dv">0</span>, <span class="at">adjust =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<p><strong>Multivariate Adaptive Regression Splines (MARS)</strong></p>
<p>MARS! This one is outta this world! This approach essentially uses piecewise regression to split into linear/non-linear patterns for each piece. Each split is a “knot” with more knots = more lines. MARS avoid overfitting with the knots by using generalized cross validation (GCV), a fast cross validation method.</p>
<p>We see from the model summary that 6 of the 32 variables were used, which appear to be differences in: total rebounds, personal fouls, offensive rebounds, adjusted efficiency margin, BARTHAG, and wins above bubble. The variable importance printout also highlights the number of subsets we saw these variables occur in, so this model supports the adjusted efficiency margin is most important (just like the other models have supported as well).</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="index.html#cb40-1" tabindex="-1"></a>mars2 <span class="ot">&lt;-</span> <span class="fu">earth</span>(Winner_A <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">glm =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span> <span class="st">&quot;logit&quot;</span>)))</span>
<span id="cb40-2"><a href="index.html#cb40-2" tabindex="-1"></a><span class="fu">summary</span>(mars2)</span></code></pre></div>
<pre><code>## Call: earth(formula=Winner_A~., data=train, glm=list(family=binomial(link=&quot;logit&quot;)))
## 
## GLM coefficients
##                                            1
## (Intercept)                        3.1504010
## h(Diff_TRB_PRE- -5.3)             -1.6088550
## h(Diff_TRB_PRE- -3.3)              1.8979659
## h(Diff_TRB_PRE- -0.1)             -0.4710103
## h(Diff_PF_PRE-2.8)                -0.5111005
## h(-4.9-Diff_OReb_PRE)             -4.3525707
## h(Diff_OReb_PRE- -3.2)             0.2433791
## h(-0.611326-Diff_AdjEM_PRE)       -0.4565465
## h(Diff_AdjEM_PRE- -0.611326)       0.1670939
## h(Diff_BARTHAG_PRE- -0.00739813) -24.7812975
## h(Diff_BARTHAG_PRE-0.0614952)     21.8683171
## h(6.82281-Diff_WAB_PRE)           -0.1084975
## 
## GLM (family binomial, link logit):
##  nulldev  df       dev  df   devratio     AIC iters converged
##  1063.68 880   875.651 869      0.177   899.7     5         1
## 
## Earth selected 12 of 53 terms, and 6 of 32 predictors
## Termination condition: Reached nk 65
## Importance: Diff_AdjEM_PRE, Diff_TRB_PRE, Diff_OReb_PRE, Diff_WAB_PRE, ...
## Number of terms at each degree of interaction: 1 11 (additive model)
## Earth GCV 0.1768966    RSS 147.8149    GRSq 0.1457851    RSq 0.187962</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="index.html#cb42-1" tabindex="-1"></a><span class="fu">evimp</span>(mars2)</span></code></pre></div>
<pre><code>##                       nsubsets   gcv    rss
## Diff_AdjEM_PRE              11 100.0  100.0
## Diff_TRB_PRE                 7  40.7   51.8
## Diff_OReb_PRE                6  40.5   49.6
## Diff_WAB_PRE                 6  36.5   47.3
## Diff_BARTHAG_PRE             4  28.5   37.7
## Diff_OppTO_PRE-unused        2  23.2   28.9
## Diff_PF_PRE                  1  11.0   17.0</code></pre>
<p><strong>Weighted XGBoost</strong></p>
<p>Here, I am looking to evaluate the XGBoost model with weights on various rows. For example, I thought that putting a higher weight on the Round of 64 matches to put a stronger priority on getting those picks right, might help make the model succeed later on for later rounds of the tournament.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="index.html#cb44-1" tabindex="-1"></a><span class="do">######### CV to minimize number of trees --&gt; 4 or 5</span></span>
<span id="cb44-2"><a href="index.html#cb44-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb44-3"><a href="index.html#cb44-3" tabindex="-1"></a>xgbcv.mm.w <span class="ot">&lt;-</span> <span class="fu">xgb.cv</span>(<span class="at">data =</span> train_x, <span class="at">label =</span> train_y, </span>
<span id="cb44-4"><a href="index.html#cb44-4" tabindex="-1"></a>                   <span class="at">nrounds =</span> <span class="dv">50</span>, <span class="at">nfold =</span> <span class="dv">10</span>, <span class="at">weight =</span> weights)</span></code></pre></div>
<pre><code>## [1]  train-rmse:0.430903+0.003096    test-rmse:0.468932+0.013417 
## [2]  train-rmse:0.384286+0.005898    test-rmse:0.453792+0.020092 
## [3]  train-rmse:0.345381+0.005273    test-rmse:0.445784+0.021019 
## [4]  train-rmse:0.316384+0.006835    test-rmse:0.446950+0.022418 
## [5]  train-rmse:0.293757+0.009228    test-rmse:0.446562+0.025600 
## [6]  train-rmse:0.273030+0.009880    test-rmse:0.451337+0.027964 
## [7]  train-rmse:0.256176+0.007530    test-rmse:0.452004+0.025227 
## [8]  train-rmse:0.241149+0.008816    test-rmse:0.455333+0.024287 
## [9]  train-rmse:0.228281+0.010449    test-rmse:0.456565+0.024094 
## [10] train-rmse:0.216687+0.010180    test-rmse:0.457721+0.026740 
## [11] train-rmse:0.202741+0.010538    test-rmse:0.459428+0.028016 
## [12] train-rmse:0.194377+0.010288    test-rmse:0.461361+0.029511 
## [13] train-rmse:0.181640+0.009770    test-rmse:0.463641+0.029259 
## [14] train-rmse:0.171866+0.012194    test-rmse:0.464514+0.030984 
## [15] train-rmse:0.163150+0.012990    test-rmse:0.464894+0.032071 
## [16] train-rmse:0.154603+0.011765    test-rmse:0.464739+0.031388 
## [17] train-rmse:0.146318+0.011443    test-rmse:0.465276+0.031111 
## [18] train-rmse:0.138805+0.011136    test-rmse:0.465308+0.031239 
## [19] train-rmse:0.129978+0.011959    test-rmse:0.465801+0.030920 
## [20] train-rmse:0.124520+0.010178    test-rmse:0.465932+0.031246 
## [21] train-rmse:0.117952+0.009996    test-rmse:0.466556+0.031030 
## [22] train-rmse:0.111825+0.010917    test-rmse:0.466651+0.031172 
## [23] train-rmse:0.107445+0.010705    test-rmse:0.466384+0.031661 
## [24] train-rmse:0.103064+0.010074    test-rmse:0.466364+0.031823 
## [25] train-rmse:0.099581+0.009581    test-rmse:0.467014+0.032270 
## [26] train-rmse:0.095859+0.009939    test-rmse:0.467344+0.032388 
## [27] train-rmse:0.090616+0.009554    test-rmse:0.467985+0.032904 
## [28] train-rmse:0.087065+0.009176    test-rmse:0.468165+0.032752 
## [29] train-rmse:0.083210+0.007993    test-rmse:0.468050+0.033065 
## [30] train-rmse:0.079087+0.007444    test-rmse:0.468581+0.032876 
## [31] train-rmse:0.074959+0.007446    test-rmse:0.469065+0.032656 
## [32] train-rmse:0.071553+0.007557    test-rmse:0.468574+0.032335 
## [33] train-rmse:0.067620+0.006159    test-rmse:0.469488+0.031538 
## [34] train-rmse:0.064365+0.006136    test-rmse:0.469474+0.031288 
## [35] train-rmse:0.061643+0.006000    test-rmse:0.469313+0.030902 
## [36] train-rmse:0.058797+0.006732    test-rmse:0.469471+0.030762 
## [37] train-rmse:0.055419+0.006812    test-rmse:0.469968+0.030828 
## [38] train-rmse:0.053188+0.007051    test-rmse:0.470002+0.030991 
## [39] train-rmse:0.050585+0.006540    test-rmse:0.470589+0.031112 
## [40] train-rmse:0.047737+0.006093    test-rmse:0.470679+0.031326 
## [41] train-rmse:0.045689+0.006167    test-rmse:0.470848+0.031427 
## [42] train-rmse:0.043467+0.006012    test-rmse:0.471079+0.031433 
## [43] train-rmse:0.041277+0.005449    test-rmse:0.471322+0.031613 
## [44] train-rmse:0.038859+0.004822    test-rmse:0.471307+0.031817 
## [45] train-rmse:0.037077+0.005037    test-rmse:0.471415+0.031865 
## [46] train-rmse:0.035567+0.005166    test-rmse:0.471054+0.032133 
## [47] train-rmse:0.033748+0.005247    test-rmse:0.471261+0.032097 
## [48] train-rmse:0.032073+0.005383    test-rmse:0.471253+0.032208 
## [49] train-rmse:0.030420+0.004560    test-rmse:0.471464+0.032281 
## [50] train-rmse:0.029104+0.004334    test-rmse:0.471418+0.032343</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="index.html#cb46-1" tabindex="-1"></a><span class="do">############# grid search to optimize parameters</span></span>
<span id="cb46-2"><a href="index.html#cb46-2" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb46-3"><a href="index.html#cb46-3" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">3</span>, <span class="co">#c(4:6),</span></span>
<span id="cb46-4"><a href="index.html#cb46-4" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>,<span class="fl">0.4</span>, <span class="fl">0.45</span>),</span>
<span id="cb46-5"><a href="index.html#cb46-5" tabindex="-1"></a>  <span class="at">max_depth =</span> <span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>),</span>
<span id="cb46-6"><a href="index.html#cb46-6" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">c</span>(<span class="dv">0</span>),</span>
<span id="cb46-7"><a href="index.html#cb46-7" tabindex="-1"></a>  <span class="at">colsample_bytree =</span> <span class="fu">c</span>(<span class="fl">0.9</span>, <span class="dv">1</span>),</span>
<span id="cb46-8"><a href="index.html#cb46-8" tabindex="-1"></a>  <span class="at">min_child_weight =</span> <span class="dv">1</span>,</span>
<span id="cb46-9"><a href="index.html#cb46-9" tabindex="-1"></a>  <span class="at">subsample =</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="dv">1</span>)</span>
<span id="cb46-10"><a href="index.html#cb46-10" tabindex="-1"></a>)</span>
<span id="cb46-11"><a href="index.html#cb46-11" tabindex="-1"></a></span>
<span id="cb46-12"><a href="index.html#cb46-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>) <span class="co">#</span></span>
<span id="cb46-13"><a href="index.html#cb46-13" tabindex="-1"></a>xgb.ins.caret <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(<span class="at">x =</span> train_x, <span class="at">y =</span> train_y,</span>
<span id="cb46-14"><a href="index.html#cb46-14" tabindex="-1"></a>                              <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb46-15"><a href="index.html#cb46-15" tabindex="-1"></a>                              <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb46-16"><a href="index.html#cb46-16" tabindex="-1"></a>                              <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="co"># Using 10-fold cross-validation</span></span>
<span id="cb46-17"><a href="index.html#cb46-17" tabindex="-1"></a>                                                       <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb46-18"><a href="index.html#cb46-18" tabindex="-1"></a>                              <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb46-19"><a href="index.html#cb46-19" tabindex="-1"></a>                              <span class="at">weights =</span> weights,</span>
<span id="cb46-20"><a href="index.html#cb46-20" tabindex="-1"></a>                              <span class="at">verbosity =</span><span class="dv">0</span>) <span class="co">#gets rid of warning messages</span></span>
<span id="cb46-21"><a href="index.html#cb46-21" tabindex="-1"></a></span>
<span id="cb46-22"><a href="index.html#cb46-22" tabindex="-1"></a>xgb.ins.caret<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 73       3         2 0.4     0              0.9                1       0.8</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="index.html#cb48-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb48-2"><a href="index.html#cb48-2" tabindex="-1"></a>xgb.mm.w <span class="ot">&lt;-</span> <span class="fu">xgboost</span>(<span class="at">data =</span> train_x, <span class="at">label =</span> train_y, </span>
<span id="cb48-3"><a href="index.html#cb48-3" tabindex="-1"></a>                  <span class="at">nrounds =</span> <span class="dv">3</span>, <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb48-4"><a href="index.html#cb48-4" tabindex="-1"></a>                  <span class="at">max_depth =</span> <span class="dv">2</span>, <span class="at">eta =</span> <span class="fl">0.45</span>, <span class="at">subsample =</span> <span class="dv">1</span>, <span class="at">weight =</span> weights)</span></code></pre></div>
<pre><code>## [1]  train-logloss:0.592581 
## [2]  train-logloss:0.548459 
## [3]  train-logloss:0.526178</code></pre>
</div>
<div id="model-comparison" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Model Comparison<a href="index.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that I have built these models, let’s compare on the validation sets. I separately calculated the logistic regression AUC’s in code chunks above since there were two separate datasets.</p>
<p>The main takeaways from this output are that the MARS, XGBoost/Weighted XGBoost, and Random Forest models perform the best. It is interesting to see such a high AUC for the SVM model on the training set, but you can see as it sees new data in the validation set it does much worse (hence, overfitting).</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="index.html#cb50-1" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Model =</span> <span class="fu">character</span>(),</span>
<span id="cb50-2"><a href="index.html#cb50-2" tabindex="-1"></a>                            <span class="at">Train_AUC =</span> <span class="fu">numeric</span>(),</span>
<span id="cb50-3"><a href="index.html#cb50-3" tabindex="-1"></a>                            <span class="at">Valid_AUC =</span> <span class="fu">numeric</span>())</span>
<span id="cb50-4"><a href="index.html#cb50-4" tabindex="-1"></a><span class="co">#log reg</span></span>
<span id="cb50-5"><a href="index.html#cb50-5" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;Logistic Regression&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb50-6"><a href="index.html#cb50-6" tabindex="-1"></a></span>
<span id="cb50-7"><a href="index.html#cb50-7" tabindex="-1"></a><span class="co">#xgboost</span></span>
<span id="cb50-8"><a href="index.html#cb50-8" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> train_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb50-9"><a href="index.html#cb50-9" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, train<span class="sc">$</span>Winner_A)</span>
<span id="cb50-10"><a href="index.html#cb50-10" tabindex="-1"></a></span>
<span id="cb50-11"><a href="index.html#cb50-11" tabindex="-1"></a>conditional2023_x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>., <span class="at">data =</span> conditional2023[<span class="dv">6</span><span class="sc">:</span><span class="dv">37</span>])[,<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>)]</span>
<span id="cb50-12"><a href="index.html#cb50-12" tabindex="-1"></a>conditional2023<span class="sc">$</span>espnXGB <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> conditional2023_x, <span class="at">type =</span> <span class="st">&#39;prob&#39;</span>)</span>
<span id="cb50-13"><a href="index.html#cb50-13" tabindex="-1"></a>conditional2023<span class="sc">$</span>winnerXGB <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(conditional2023<span class="sc">$</span>espnXGB <span class="sc">&gt;</span> <span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal[<span class="dv">4</span>],<span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="index.html#cb51-1" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> valid_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb51-2"><a href="index.html#cb51-2" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, valid<span class="sc">$</span>Winner_A)</span>
<span id="cb51-3"><a href="index.html#cb51-3" tabindex="-1"></a></span>
<span id="cb51-4"><a href="index.html#cb51-4" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;XGBoost&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb51-5"><a href="index.html#cb51-5" tabindex="-1"></a></span>
<span id="cb51-6"><a href="index.html#cb51-6" tabindex="-1"></a><span class="co">#random forest</span></span>
<span id="cb51-7"><a href="index.html#cb51-7" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.mm, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb51-8"><a href="index.html#cb51-8" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">2</span>],train<span class="sc">$</span>Winner_A)</span>
<span id="cb51-9"><a href="index.html#cb51-9" tabindex="-1"></a></span>
<span id="cb51-10"><a href="index.html#cb51-10" tabindex="-1"></a>conditional2023<span class="sc">$</span>espnRF <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.mm, <span class="at">newdata =</span> conditional2023, <span class="at">type =</span> <span class="st">&#39;prob&#39;</span>)</span>
<span id="cb51-11"><a href="index.html#cb51-11" tabindex="-1"></a>conditional2023<span class="sc">$</span>winnerRF <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(conditional2023<span class="sc">$</span>espnRF[,<span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal[<span class="dv">4</span>],<span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-40-2.png" width="672" /></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="index.html#cb52-1" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.mm, <span class="at">newdata =</span> valid, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb52-2"><a href="index.html#cb52-2" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">2</span>], valid<span class="sc">$</span>Winner_A)</span>
<span id="cb52-3"><a href="index.html#cb52-3" tabindex="-1"></a></span>
<span id="cb52-4"><a href="index.html#cb52-4" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;Random Forest&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb52-5"><a href="index.html#cb52-5" tabindex="-1"></a></span>
<span id="cb52-6"><a href="index.html#cb52-6" tabindex="-1"></a><span class="co">#mars</span></span>
<span id="cb52-7"><a href="index.html#cb52-7" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> train, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb52-8"><a href="index.html#cb52-8" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">1</span>], train<span class="sc">$</span>Winner_A)</span>
<span id="cb52-9"><a href="index.html#cb52-9" tabindex="-1"></a></span>
<span id="cb52-10"><a href="index.html#cb52-10" tabindex="-1"></a>conditional2023<span class="sc">$</span>espnMARS <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> conditional2023, <span class="at">type =</span> <span class="st">&#39;response&#39;</span>)</span>
<span id="cb52-11"><a href="index.html#cb52-11" tabindex="-1"></a>conditional2023<span class="sc">$</span>winnerMARS <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(conditional2023<span class="sc">$</span>espnMARS[,<span class="dv">1</span>] <span class="sc">&gt;</span> <span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal[<span class="dv">4</span>],<span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-40-3.png" width="672" /></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="index.html#cb53-1" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> valid, <span class="at">type =</span> <span class="st">&#39;response&#39;</span>)</span>
<span id="cb53-2"><a href="index.html#cb53-2" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">1</span>], valid<span class="sc">$</span>Winner_A)</span>
<span id="cb53-3"><a href="index.html#cb53-3" tabindex="-1"></a></span>
<span id="cb53-4"><a href="index.html#cb53-4" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;MARS&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb53-5"><a href="index.html#cb53-5" tabindex="-1"></a></span>
<span id="cb53-6"><a href="index.html#cb53-6" tabindex="-1"></a><span class="co">#svm</span></span>
<span id="cb53-7"><a href="index.html#cb53-7" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm.mm, <span class="at">newdata =</span> train_x)</span>
<span id="cb53-8"><a href="index.html#cb53-8" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, train<span class="sc">$</span>Winner_A)</span>
<span id="cb53-9"><a href="index.html#cb53-9" tabindex="-1"></a></span>
<span id="cb53-10"><a href="index.html#cb53-10" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm.mm, <span class="at">newdata =</span> valid_x, <span class="at">probability =</span> <span class="cn">TRUE</span>)</span>
<span id="cb53-11"><a href="index.html#cb53-11" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, valid<span class="sc">$</span>Winner_A)</span>
<span id="cb53-12"><a href="index.html#cb53-12" tabindex="-1"></a></span>
<span id="cb53-13"><a href="index.html#cb53-13" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;SVM&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb53-14"><a href="index.html#cb53-14" tabindex="-1"></a></span>
<span id="cb53-15"><a href="index.html#cb53-15" tabindex="-1"></a><span class="co">#naive bayes</span></span>
<span id="cb53-16"><a href="index.html#cb53-16" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.mm, <span class="at">newdata =</span> train, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>)</span>
<span id="cb53-17"><a href="index.html#cb53-17" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">2</span>], train<span class="sc">$</span>Winner_A)</span>
<span id="cb53-18"><a href="index.html#cb53-18" tabindex="-1"></a></span>
<span id="cb53-19"><a href="index.html#cb53-19" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.mm, <span class="at">newdata =</span> valid, <span class="at">type =</span> <span class="st">&#39;raw&#39;</span>)</span>
<span id="cb53-20"><a href="index.html#cb53-20" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">2</span>], valid<span class="sc">$</span>Winner_A)</span>
<span id="cb53-21"><a href="index.html#cb53-21" tabindex="-1"></a></span>
<span id="cb53-22"><a href="index.html#cb53-22" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;Naive Bayes&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb53-23"><a href="index.html#cb53-23" tabindex="-1"></a></span>
<span id="cb53-24"><a href="index.html#cb53-24" tabindex="-1"></a><span class="co">#weighted xgboost</span></span>
<span id="cb53-25"><a href="index.html#cb53-25" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm.w, <span class="at">newdata =</span> train_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb53-26"><a href="index.html#cb53-26" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, train<span class="sc">$</span>Winner_A)</span>
<span id="cb53-27"><a href="index.html#cb53-27" tabindex="-1"></a></span>
<span id="cb53-28"><a href="index.html#cb53-28" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm.w, <span class="at">newdata =</span> valid_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb53-29"><a href="index.html#cb53-29" tabindex="-1"></a>logit_roc_v <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, valid<span class="sc">$</span>Winner_A)</span>
<span id="cb53-30"><a href="index.html#cb53-30" tabindex="-1"></a></span>
<span id="cb53-31"><a href="index.html#cb53-31" tabindex="-1"></a>model_eval_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(model_eval_df, <span class="fu">c</span>(<span class="st">&quot;Weighted XGBoost&quot;</span>, logit_roc<span class="sc">$</span>AUC, logit_roc_v<span class="sc">$</span>AUC))</span>
<span id="cb53-32"><a href="index.html#cb53-32" tabindex="-1"></a></span>
<span id="cb53-33"><a href="index.html#cb53-33" tabindex="-1"></a><span class="fu">colnames</span>(model_eval_df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;Model&#39;</span>, <span class="st">&#39;Train AUC&#39;</span>, <span class="st">&#39;Validation AUC&#39;</span>)</span>
<span id="cb53-34"><a href="index.html#cb53-34" tabindex="-1"></a></span>
<span id="cb53-35"><a href="index.html#cb53-35" tabindex="-1"></a>model_eval_df <span class="sc">%&gt;%</span></span>
<span id="cb53-36"><a href="index.html#cb53-36" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(<span class="st">`</span><span class="at">Validation AUC</span><span class="st">`</span>))</span></code></pre></div>
<pre><code>##                 Model         Train AUC    Validation AUC
## 1                MARS 0.771251122418438 0.626794258373206
## 2    Weighted XGBoost 0.753691509528086 0.626196172248804
## 3             XGBoost 0.748858874588447 0.622009569377991
## 4       Random Forest 0.682667365060361 0.619617224880383
## 5                 SVM 0.896369599920184 0.599282296650718
## 6 Logistic Regression 0.743486855232964 0.562799043062201
## 7         Naive Bayes 0.703837423924973 0.516148325358852</code></pre>
</div>
<div id="results" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Results<a href="index.html#results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After evaluating how well the model performs on the validation set (2023 bracket) after fitting it to the training (2008-2022 brackets), I am interested in how well it performs in terms of the March Madness brackets, which is total points. Therefore, I will run the models on every conditional matchup for the year 2023 (as mentioned earlier) to test what the corresponding ESPN points will be from the best models.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="index.html#cb55-1" tabindex="-1"></a>conditional2023_best <span class="ot">&lt;-</span> <span class="fu">cbind</span>(conditional2023[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>], conditional2023<span class="sc">$</span>winnerLogReg, conditional2023<span class="sc">$</span>winnerRF, conditional2023<span class="sc">$</span>winnerXGB, conditional2023<span class="sc">$</span>winnerMARS)</span>
<span id="cb55-2"><a href="index.html#cb55-2" tabindex="-1"></a></span>
<span id="cb55-3"><a href="index.html#cb55-3" tabindex="-1"></a><span class="fu">colnames</span>(conditional2023_best) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Year&quot;</span>, <span class="st">&quot;Team A&quot;</span>, <span class="st">&quot;Team B&quot;</span>, <span class="st">&quot;A_Seed&quot;</span>, <span class="st">&quot;B_Seed&quot;</span>, <span class="st">&quot;Logistic Regression Pred&quot;</span>, <span class="st">&quot;Random Forest Pred&quot;</span>, <span class="st">&quot;XGBoost Pred&quot;</span>, <span class="st">&quot;MARS Pred&quot;</span> )</span>
<span id="cb55-4"><a href="index.html#cb55-4" tabindex="-1"></a></span>
<span id="cb55-5"><a href="index.html#cb55-5" tabindex="-1"></a><span class="fu">head</span>(conditional2023_best[<span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">9</span>)],<span class="dv">10</span>)</span></code></pre></div>
<pre><code>##     Team A          Team B A_Seed B_Seed Logistic Regression Pred Random Forest Pred
## 1  Alabama        Maryland      1      8                        1                  1
## 2  Alabama San Diego State      1      5                        0                  0
## 3  Alabama        Virginia      1      4                        0                  0
## 4  Alabama       Creighton      1      6                        0                  0
## 5  Alabama          Baylor      1      3                        0                  0
## 6  Alabama        Missouri      1      7                        0                  0
## 7  Alabama         Arizona      1      2                        0                  0
## 8  Alabama          Purdue      1      1                        0                  0
## 9  Alabama         Memphis      1      8                        0                  0
## 10 Alabama            Duke      1      5                        0                  0
##    XGBoost Pred MARS Pred
## 1             1         0
## 2             0         0
## 3             1         0
## 4             1         0
## 5             0         0
## 6             1         0
## 7             0         0
## 8             0         0
## 9             0         0
## 10            1         0</code></pre>
<p>As you can see there were tons of various matchup possibilities within the tournament and the best models I found from their AUC’s were now being used to evaluate ESPN performance. I manually calculated these results from the above table on the 2023 bracket. It’s interesting to note how the models varied, specifically a model like MARS which frequently favored upsets over dominant tournament teams.</p>
<p>Since the XGBoost and MARS models performed well on both validation AUC and ESPN scores, these were the two types of models I decided to use for making my bracket!</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="index.html#cb57-1" tabindex="-1"></a>espn_results_mod <span class="sc">%&gt;%</span></span>
<span id="cb57-2"><a href="index.html#cb57-2" tabindex="-1"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(<span class="st">`</span><span class="at">ESPN Score</span><span class="st">`</span>))</span></code></pre></div>
<pre><code>##                 Model ESPN Score
## 1             XGBoost        710
## 2                MARS        700
## 3 Logistic Regression        430
## 4       Random Forest        310</code></pre>
<p><strong>2024 Predictions</strong></p>
<p>Since I created two different models, I had two different brackets. My final four predictions for the 2024 tournament are as follows:</p>
<p>XGBoost <strong>(predicted champion: Houston)</strong></p>
<ul>
<li><p>BYU</p></li>
<li><p>New Mexico</p></li>
<li><p>Houston</p></li>
<li><p>Oregon</p></li>
</ul>
<p>MARS <strong>(predicted champion: Houston)</strong></p>
<ul>
<li><p>UConn</p></li>
<li><p>Baylor</p></li>
<li><p>Houston</p></li>
<li><p>Purdue</p></li>
</ul>
<p><em>Post Tourney Update:</em> As we can see, a lot of my picks were not correct. The MARS model scored 870 (69% correct), while the XGBoost picked too many upsets with a 360 (12.9% correct). I feel that since UConn won this year, a lot of people chose them as champion to repeat, which made for inflated points across the board. It almost wasn’t safe to NOT pick them.</p>
<p>Some interesting results is that I did have NC State in my Elite 8 for the XGBoost model, but also 2 of my Final 4 teams lost on Day 1 (leading to why that model did so poorly). The MARS model has 3/8 Elite 8 teams correct and the XGBoost had 1/8.</p>
<p>Houston had a lot of injuries in the tournament which is ultimately why they lost to duke in the Sweet 16. It was unfortunate because they had a good team this year, but it also highlights a major flaw in my model, which is that it doesn’t account for injuries.</p>
<p><strong>Testing on 2024 data</strong></p>
<p>The code below re-runs the two best models on the 2024 set by combining my 2008-2022 and 2023 validation set into one, ultimately assisting in my predictions for the 2024 tournament.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="index.html#cb59-1" tabindex="-1"></a><span class="do">###################################################################################### Re-run on combined train/test</span></span>
<span id="cb59-2"><a href="index.html#cb59-2" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb59-3"><a href="index.html#cb59-3" tabindex="-1"></a></span>
<span id="cb59-4"><a href="index.html#cb59-4" tabindex="-1"></a>mm_data <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;march_madness.xlsx&quot;</span>, <span class="at">sheet =</span> <span class="dv">2</span>)</span>
<span id="cb59-5"><a href="index.html#cb59-5" tabindex="-1"></a></span>
<span id="cb59-6"><a href="index.html#cb59-6" tabindex="-1"></a><span class="co">#only want 2008-2023</span></span>
<span id="cb59-7"><a href="index.html#cb59-7" tabindex="-1"></a>train <span class="ot">&lt;-</span> mm_data <span class="sc">%&gt;%</span></span>
<span id="cb59-8"><a href="index.html#cb59-8" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(<span class="sc">!</span>(Year <span class="sc">%in%</span> <span class="dv">2000</span><span class="sc">:</span><span class="dv">2007</span>))</span>
<span id="cb59-9"><a href="index.html#cb59-9" tabindex="-1"></a></span>
<span id="cb59-10"><a href="index.html#cb59-10" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;march_madness.xlsx&quot;</span>, <span class="at">sheet =</span> <span class="dv">7</span>)</span>
<span id="cb59-11"><a href="index.html#cb59-11" tabindex="-1"></a></span>
<span id="cb59-12"><a href="index.html#cb59-12" tabindex="-1"></a>train<span class="sc">$</span>Year <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>Year)</span>
<span id="cb59-13"><a href="index.html#cb59-13" tabindex="-1"></a>train<span class="sc">$</span>TeamA <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>TeamA)</span>
<span id="cb59-14"><a href="index.html#cb59-14" tabindex="-1"></a>train<span class="sc">$</span>TeamB <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>TeamB)</span>
<span id="cb59-15"><a href="index.html#cb59-15" tabindex="-1"></a>train<span class="sc">$</span>A_Seed <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>A_Seed)</span>
<span id="cb59-16"><a href="index.html#cb59-16" tabindex="-1"></a>train<span class="sc">$</span>B_Seed <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>B_Seed)</span>
<span id="cb59-17"><a href="index.html#cb59-17" tabindex="-1"></a>train<span class="sc">$</span>Winner_A <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>Winner_A)</span>
<span id="cb59-18"><a href="index.html#cb59-18" tabindex="-1"></a></span>
<span id="cb59-19"><a href="index.html#cb59-19" tabindex="-1"></a>train <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span></span>
<span id="cb59-20"><a href="index.html#cb59-20" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">c</span>(Year, TeamA, TeamB, A_Seed, B_Seed, A_Score, </span>
<span id="cb59-21"><a href="index.html#cb59-21" tabindex="-1"></a>                   B_Score, MOV, Round, Diff_TotalRPI_PRE, Diff_RPI_SOS_PRE))</span>
<span id="cb59-22"><a href="index.html#cb59-22" tabindex="-1"></a></span>
<span id="cb59-23"><a href="index.html#cb59-23" tabindex="-1"></a></span>
<span id="cb59-24"><a href="index.html#cb59-24" tabindex="-1"></a><span class="do">################################################</span></span>
<span id="cb59-25"><a href="index.html#cb59-25" tabindex="-1"></a><span class="do">############## XGBoost (without RPI) ##### </span></span>
<span id="cb59-26"><a href="index.html#cb59-26" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb59-27"><a href="index.html#cb59-27" tabindex="-1"></a></span>
<span id="cb59-28"><a href="index.html#cb59-28" tabindex="-1"></a><span class="do">############################################ creating the model</span></span>
<span id="cb59-29"><a href="index.html#cb59-29" tabindex="-1"></a>train_x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Winner_A <span class="sc">~</span> ., <span class="at">data =</span> train)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb59-30"><a href="index.html#cb59-30" tabindex="-1"></a>train_y <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(train<span class="sc">$</span>Winner_A)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb59-31"><a href="index.html#cb59-31" tabindex="-1"></a><span class="co">#weights &lt;- train$wts</span></span>
<span id="cb59-32"><a href="index.html#cb59-32" tabindex="-1"></a></span>
<span id="cb59-33"><a href="index.html#cb59-33" tabindex="-1"></a></span>
<span id="cb59-34"><a href="index.html#cb59-34" tabindex="-1"></a><span class="do">######### CV to minimize number of trees --&gt; 4 or 5</span></span>
<span id="cb59-35"><a href="index.html#cb59-35" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb59-36"><a href="index.html#cb59-36" tabindex="-1"></a>xgbcv.mm <span class="ot">&lt;-</span> <span class="fu">xgb.cv</span>(<span class="at">data =</span> train_x, <span class="at">label =</span> train_y, </span>
<span id="cb59-37"><a href="index.html#cb59-37" tabindex="-1"></a>                   <span class="at">nrounds =</span> <span class="dv">50</span>, <span class="at">nfold =</span> <span class="dv">10</span>) <span class="co">#, subsample = 0.8, eta = 0.4, max_depth = 2, colsample_bytree = 0.9)</span></span></code></pre></div>
<pre><code>## [1]  train-rmse:0.434727+0.003235    test-rmse:0.468224+0.008606 
## [2]  train-rmse:0.389473+0.004858    test-rmse:0.454340+0.012149 
## [3]  train-rmse:0.355154+0.007297    test-rmse:0.448911+0.016355 
## [4]  train-rmse:0.324747+0.006226    test-rmse:0.448417+0.018784 
## [5]  train-rmse:0.300825+0.006158    test-rmse:0.446723+0.019199 
## [6]  train-rmse:0.279920+0.006608    test-rmse:0.450033+0.020695 
## [7]  train-rmse:0.262114+0.008109    test-rmse:0.450797+0.020822 
## [8]  train-rmse:0.249341+0.010782    test-rmse:0.452088+0.022219 
## [9]  train-rmse:0.235380+0.010566    test-rmse:0.454445+0.023068 
## [10] train-rmse:0.222538+0.011564    test-rmse:0.455147+0.024646 
## [11] train-rmse:0.213403+0.014133    test-rmse:0.456867+0.025222 
## [12] train-rmse:0.201399+0.013004    test-rmse:0.459187+0.026086 
## [13] train-rmse:0.189349+0.011185    test-rmse:0.461680+0.024865 
## [14] train-rmse:0.179177+0.011868    test-rmse:0.461672+0.025642 
## [15] train-rmse:0.169862+0.011278    test-rmse:0.461396+0.025567 
## [16] train-rmse:0.161540+0.009038    test-rmse:0.462729+0.026368 
## [17] train-rmse:0.154318+0.008264    test-rmse:0.463559+0.025553 
## [18] train-rmse:0.147862+0.008795    test-rmse:0.464363+0.026086 
## [19] train-rmse:0.140701+0.009395    test-rmse:0.464886+0.024830 
## [20] train-rmse:0.133469+0.009780    test-rmse:0.465738+0.025398 
## [21] train-rmse:0.125221+0.009706    test-rmse:0.465138+0.024787 
## [22] train-rmse:0.119154+0.007843    test-rmse:0.464952+0.024797 
## [23] train-rmse:0.113461+0.006875    test-rmse:0.465425+0.024687 
## [24] train-rmse:0.109472+0.005950    test-rmse:0.465798+0.024827 
## [25] train-rmse:0.104213+0.007496    test-rmse:0.465263+0.025152 
## [26] train-rmse:0.098973+0.008562    test-rmse:0.465886+0.025744 
## [27] train-rmse:0.093404+0.008999    test-rmse:0.465648+0.025726 
## [28] train-rmse:0.089116+0.009743    test-rmse:0.466550+0.025393 
## [29] train-rmse:0.084757+0.009898    test-rmse:0.466454+0.025612 
## [30] train-rmse:0.080045+0.008470    test-rmse:0.466101+0.025882 
## [31] train-rmse:0.075864+0.007052    test-rmse:0.466778+0.026022 
## [32] train-rmse:0.073124+0.006530    test-rmse:0.467168+0.026207 
## [33] train-rmse:0.069672+0.005607    test-rmse:0.467927+0.026504 
## [34] train-rmse:0.066494+0.005880    test-rmse:0.468032+0.026207 
## [35] train-rmse:0.062626+0.005629    test-rmse:0.468421+0.026568 
## [36] train-rmse:0.059982+0.004963    test-rmse:0.468674+0.026662 
## [37] train-rmse:0.057864+0.004789    test-rmse:0.468946+0.027012 
## [38] train-rmse:0.055833+0.004413    test-rmse:0.468900+0.027126 
## [39] train-rmse:0.053030+0.004163    test-rmse:0.469035+0.027264 
## [40] train-rmse:0.050486+0.003541    test-rmse:0.468935+0.027284 
## [41] train-rmse:0.048677+0.003510    test-rmse:0.469158+0.027634 
## [42] train-rmse:0.046993+0.003375    test-rmse:0.469147+0.027702 
## [43] train-rmse:0.044849+0.003084    test-rmse:0.468869+0.027790 
## [44] train-rmse:0.042702+0.002674    test-rmse:0.469087+0.027781 
## [45] train-rmse:0.040682+0.002714    test-rmse:0.469260+0.028005 
## [46] train-rmse:0.038509+0.002525    test-rmse:0.469474+0.028152 
## [47] train-rmse:0.036659+0.002617    test-rmse:0.469620+0.028179 
## [48] train-rmse:0.034751+0.003065    test-rmse:0.469763+0.028186 
## [49] train-rmse:0.033110+0.003106    test-rmse:0.469973+0.028211 
## [50] train-rmse:0.031507+0.003041    test-rmse:0.470171+0.028417</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="index.html#cb61-1" tabindex="-1"></a><span class="do">############# grid search to optimize parameters</span></span>
<span id="cb61-2"><a href="index.html#cb61-2" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb61-3"><a href="index.html#cb61-3" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">5</span>, <span class="co">#c(4:6),</span></span>
<span id="cb61-4"><a href="index.html#cb61-4" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>,<span class="fl">0.4</span>, <span class="fl">0.45</span>),</span>
<span id="cb61-5"><a href="index.html#cb61-5" tabindex="-1"></a>  <span class="at">max_depth =</span> <span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>),</span>
<span id="cb61-6"><a href="index.html#cb61-6" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">c</span>(<span class="dv">0</span>),</span>
<span id="cb61-7"><a href="index.html#cb61-7" tabindex="-1"></a>  <span class="at">colsample_bytree =</span> <span class="fu">c</span>(<span class="fl">0.9</span>, <span class="dv">1</span>),</span>
<span id="cb61-8"><a href="index.html#cb61-8" tabindex="-1"></a>  <span class="at">min_child_weight =</span> <span class="dv">1</span>,</span>
<span id="cb61-9"><a href="index.html#cb61-9" tabindex="-1"></a>  <span class="at">subsample =</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.9</span>, <span class="dv">1</span>)</span>
<span id="cb61-10"><a href="index.html#cb61-10" tabindex="-1"></a>)</span>
<span id="cb61-11"><a href="index.html#cb61-11" tabindex="-1"></a></span>
<span id="cb61-12"><a href="index.html#cb61-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>) <span class="co">#</span></span>
<span id="cb61-13"><a href="index.html#cb61-13" tabindex="-1"></a>xgb.ins.caret <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(<span class="at">x =</span> train_x, <span class="at">y =</span> train_y,</span>
<span id="cb61-14"><a href="index.html#cb61-14" tabindex="-1"></a>                              <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb61-15"><a href="index.html#cb61-15" tabindex="-1"></a>                              <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb61-16"><a href="index.html#cb61-16" tabindex="-1"></a>                              <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="co"># Using 10-fold cross-validation</span></span>
<span id="cb61-17"><a href="index.html#cb61-17" tabindex="-1"></a>                                                       <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb61-18"><a href="index.html#cb61-18" tabindex="-1"></a>                              <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb61-19"><a href="index.html#cb61-19" tabindex="-1"></a>                              <span class="at">verbosity =</span><span class="dv">0</span>) <span class="co">#gets rid of warning messages</span></span>
<span id="cb61-20"><a href="index.html#cb61-20" tabindex="-1"></a></span>
<span id="cb61-21"><a href="index.html#cb61-21" tabindex="-1"></a><span class="fu">plot</span>(xgb.ins.caret)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="index.html#cb62-1" tabindex="-1"></a>xgb.ins.caret<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 75       5         2 0.4     0              0.9                1         1</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="index.html#cb64-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb64-2"><a href="index.html#cb64-2" tabindex="-1"></a>xgb.mm <span class="ot">&lt;-</span> <span class="fu">xgboost</span>(<span class="at">data =</span> train_x, <span class="at">label =</span> train_y, </span>
<span id="cb64-3"><a href="index.html#cb64-3" tabindex="-1"></a>                  <span class="at">nrounds =</span> <span class="dv">5</span>, <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>,</span>
<span id="cb64-4"><a href="index.html#cb64-4" tabindex="-1"></a>                  <span class="at">max_depth =</span> <span class="dv">2</span>, <span class="at">eta =</span> <span class="fl">0.4</span>, <span class="at">subsample =</span> <span class="dv">1</span>, <span class="at">colsample_bytree =</span> <span class="fl">0.9</span>)</span></code></pre></div>
<pre><code>## [1]  train-logloss:0.602898 
## [2]  train-logloss:0.561401 
## [3]  train-logloss:0.538805 
## [4]  train-logloss:0.525726 
## [5]  train-logloss:0.514920</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="index.html#cb66-1" tabindex="-1"></a><span class="do">#################################### AUROC comparison</span></span>
<span id="cb66-2"><a href="index.html#cb66-2" tabindex="-1"></a></span>
<span id="cb66-3"><a href="index.html#cb66-3" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> train_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb66-4"><a href="index.html#cb66-4" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, train<span class="sc">$</span>Winner_A)</span>
<span id="cb66-5"><a href="index.html#cb66-5" tabindex="-1"></a><span class="fu">plot</span>(logit_roc) <span class="co">#chance line = randomly flipped a coin to assign 0&#39;s and 1&#39;s</span></span>
<span id="cb66-6"><a href="index.html#cb66-6" tabindex="-1"></a></span>
<span id="cb66-7"><a href="index.html#cb66-7" tabindex="-1"></a><span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal  <span class="co">#cutoff = optimal cutoff that maximizes Youden index </span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-2.png" width="672" /></p>
<pre><code>##     value       FPR       TPR    cutoff 
## 0.4004599 0.1594203 0.5598802 0.7065721</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="index.html#cb68-1" tabindex="-1"></a><span class="fu">summary</span>(logit_roc)</span></code></pre></div>
<pre><code>##                            
##  Method used: empirical    
##  Number of positive(s): 668
##  Number of negative(s): 276
##  Area under curve: 0.7629</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="index.html#cb70-1" tabindex="-1"></a><span class="do">################################### variable importance</span></span>
<span id="cb70-2"><a href="index.html#cb70-2" tabindex="-1"></a></span>
<span id="cb70-3"><a href="index.html#cb70-3" tabindex="-1"></a><span class="co">#using optimal parameters from previous tuning</span></span>
<span id="cb70-4"><a href="index.html#cb70-4" tabindex="-1"></a><span class="fu">xgb.importance</span>(<span class="at">feature_names =</span> <span class="fu">colnames</span>(train_x), <span class="at">model =</span> xgb.mm)</span></code></pre></div>
<pre><code>##                         Feature       Gain      Cover  Frequency
##  1:              Diff_AdjEM_PRE 0.47047026 0.37001881 0.26666667
##  2:                Diff_WAB_PRE 0.13514857 0.10618818 0.06666667
##  3:            Diff_BARTHAG_PRE 0.09595291 0.06678718 0.06666667
##  4:               Diff_AdjD_PRE 0.09002270 0.09099804 0.13333333
##  5: Diff_OppTO_TeamTO_Ratio_PRE 0.04201116 0.06083433 0.13333333
##  6:        Diff_UnadjOffEff_PRE 0.03802628 0.05399847 0.06666667
##  7:                Diff_STL_PRE 0.03705511 0.08931092 0.06666667
##  8:               Diff_AdjO_PRE 0.03292359 0.05050259 0.06666667
##  9:   Diff_AdjT_AdjEM_Ratio_PRE 0.02986490 0.07704467 0.06666667
## 10:                Diff_BLK_PRE 0.02852452 0.03431682 0.06666667</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="index.html#cb72-1" tabindex="-1"></a><span class="co">#will automatically cluster variables statistically based on similar gain</span></span>
<span id="cb72-2"><a href="index.html#cb72-2" tabindex="-1"></a><span class="fu">xgb.ggplot.importance</span>(<span class="fu">xgb.importance</span>(<span class="at">feature_names =</span> <span class="fu">colnames</span>(train_x), <span class="at">model =</span> xgb.mm))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-3.png" width="672" /></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="index.html#cb73-1" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb73-2"><a href="index.html#cb73-2" tabindex="-1"></a><span class="do">####### making predictions on validation set </span></span>
<span id="cb73-3"><a href="index.html#cb73-3" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb73-4"><a href="index.html#cb73-4" tabindex="-1"></a></span>
<span id="cb73-5"><a href="index.html#cb73-5" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> valid_x, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb73-6"><a href="index.html#cb73-6" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat, valid<span class="sc">$</span>Winner_A)</span>
<span id="cb73-7"><a href="index.html#cb73-7" tabindex="-1"></a><span class="fu">plot</span>(logit_roc) <span class="co">#chance line = randomly flipped a coin to assign 0&#39;s and 1&#39;s</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-4.png" width="672" /></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="index.html#cb74-1" tabindex="-1"></a><span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal  <span class="co">#cutoff = optimal cutoff that maximizes Youden index </span></span></code></pre></div>
<pre><code>##     value       FPR       TPR    cutoff 
## 0.2966507 0.1578947 0.4545455 0.7253609</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="index.html#cb76-1" tabindex="-1"></a><span class="fu">summary</span>(logit_roc)</span></code></pre></div>
<pre><code>##                           
##  Method used: empirical   
##  Number of positive(s): 44
##  Number of negative(s): 19
##  Area under curve: 0.6226</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="index.html#cb78-1" tabindex="-1"></a>XGB <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(p_hat<span class="sc">&gt;</span><span class="fl">0.6957421</span> ,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb78-2"><a href="index.html#cb78-2" tabindex="-1"></a></span>
<span id="cb78-3"><a href="index.html#cb78-3" tabindex="-1"></a></span>
<span id="cb78-4"><a href="index.html#cb78-4" tabindex="-1"></a>test_x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>.<span class="sc">-</span>Year<span class="sc">-</span>TeamA<span class="sc">-</span>TeamB<span class="sc">-</span>A_Seed<span class="sc">-</span>B_Seed, <span class="at">data =</span> test)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb78-5"><a href="index.html#cb78-5" tabindex="-1"></a>test<span class="sc">$</span>espnXGB <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb.mm, <span class="at">newdata =</span> test_x, <span class="at">type =</span> <span class="st">&#39;prob&#39;</span>)</span>
<span id="cb78-6"><a href="index.html#cb78-6" tabindex="-1"></a>test<span class="sc">$</span>winnerXGB <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(test<span class="sc">$</span>espnXGB <span class="sc">&gt;</span> <span class="fl">0.7065721</span>,<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb78-7"><a href="index.html#cb78-7" tabindex="-1"></a></span>
<span id="cb78-8"><a href="index.html#cb78-8" tabindex="-1"></a>test[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">37</span><span class="sc">:</span><span class="dv">39</span>)]</span></code></pre></div>
<pre><code>## # A tibble: 4,095 × 8
##     Year TeamA TeamB      A_Seed B_Seed Diff_OppTO_TeamTO_Ratio_PRE espnXGB winnerXGB
##    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;                       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
##  1  2024 UConn Purdue          1      1                      0.229    0.607         0
##  2  2024 UConn UNC             1      1                      0.0420   0.697         0
##  3  2024 UConn Houston         1      1                     -0.706    0.560         0
##  4  2024 UConn Iowa State      1      2                     -0.551    0.667         0
##  5  2024 UConn Arizona         1      2                     -0.0442   0.597         0
##  6  2024 UConn Marquette       1      2                     -0.404    0.597         0
##  7  2024 UConn Tennessee       1      2                     -0.210    0.597         0
##  8  2024 UConn Illinois        1      3                      0.266    0.707         1
##  9  2024 UConn Kentucky        1      3                     -0.0407   0.847         1
## 10  2024 UConn Baylor          1      3                      0.114    0.727         1
## # ℹ 4,085 more rows</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="index.html#cb80-1" tabindex="-1"></a><span class="do">################################################</span></span>
<span id="cb80-2"><a href="index.html#cb80-2" tabindex="-1"></a><span class="do">############## MARS (without RPI) #####</span></span>
<span id="cb80-3"><a href="index.html#cb80-3" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb80-4"><a href="index.html#cb80-4" tabindex="-1"></a></span>
<span id="cb80-5"><a href="index.html#cb80-5" tabindex="-1"></a></span>
<span id="cb80-6"><a href="index.html#cb80-6" tabindex="-1"></a>mars2 <span class="ot">&lt;-</span> <span class="fu">earth</span>(Winner_A <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">glm =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span> <span class="st">&quot;logit&quot;</span>)))</span>
<span id="cb80-7"><a href="index.html#cb80-7" tabindex="-1"></a><span class="fu">summary</span>(mars2)</span></code></pre></div>
<pre><code>## Call: earth(formula=Winner_A~., data=train, glm=list(family=binomial(link=&quot;logit&quot;)))
## 
## GLM coefficients
##                                            1
## (Intercept)                       22.6223795
## h(Diff_TRB_PRE- -5.5)             -1.1364572
## h(Diff_TRB_PRE- -2.7)              3.2031813
## h(Diff_TRB_PRE- -2)               -2.2016186
## h(Diff_PF_PRE-2)                  -0.3749707
## h(0.086-Diff_WLPct_PRE)            2.3280569
## h(-4.9-Diff_OReb_PRE)             -3.6841460
## h(Diff_OReb_PRE- -3.2)             0.2062965
## h(23.381-Diff_AdjO_PRE)           -0.4641799
## h(Diff_AdjD_PRE- -17.2848)        -0.5041243
## h(Diff_AdjEM_PRE- -0.435608)      -0.2766438
## h(Diff_BARTHAG_PRE- -0.00659124) -29.4664708
## h(Diff_BARTHAG_PRE-0.0645029)     25.1247938
## h(7.12517-Diff_WAB_PRE)           -0.1143358
## 
## GLM (family binomial, link logit):
##  nulldev  df       dev  df   devratio     AIC iters converged
##  1140.85 943   945.888 930      0.171   973.9     5         1
## 
## Earth selected 14 of 52 terms, and 9 of 32 predictors
## Termination condition: Reached nk 65
## Importance: Diff_AdjEM_PRE, Diff_AdjO_PRE, Diff_AdjD_PRE, Diff_TRB_PRE, ...
## Number of terms at each degree of interaction: 1 13 (additive model)
## Earth GCV 0.1787891    RSS 159.2604    GRSq 0.1376593    RSq 0.1845559</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="index.html#cb82-1" tabindex="-1"></a><span class="fu">evimp</span>(mars2)</span></code></pre></div>
<pre><code>##                                       nsubsets   gcv    rss
## Diff_AdjEM_PRE                              13 100.0  100.0
## Diff_AdjO_PRE                                9  48.4   58.9
## Diff_AdjD_PRE                                9  48.4   58.9
## Diff_TRB_PRE                                 8  46.2   55.8
## Diff_OReb_PRE                                7  38.7   49.5
## Diff_BARTHAG_PRE                             5  27.6   39.0
## Diff_PF_PRE                                  3  21.0   29.9
## Diff_WAB_PRE                                 3  18.2   28.8
## Diff_ExtraScoringChancesPG_PRE-unused        2  -9.0   18.6
## Diff_WLPct_PRE                               1   5.8   14.7</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="index.html#cb84-1" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> train, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb84-2"><a href="index.html#cb84-2" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">1</span>], train<span class="sc">$</span>Winner_A)</span>
<span id="cb84-3"><a href="index.html#cb84-3" tabindex="-1"></a><span class="fu">plot</span>(logit_roc) <span class="co">#chance line = randomly flipped a coin to assign 0&#39;s and 1&#39;s</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-5.png" width="672" /></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="index.html#cb85-1" tabindex="-1"></a><span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal  <span class="co">#cutoff = optimal cutoff that maximizes Youden index </span></span></code></pre></div>
<pre><code>##     value       FPR       TPR    cutoff 
## 0.4181420 0.2644928 0.6826347 0.6996445</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="index.html#cb87-1" tabindex="-1"></a><span class="fu">summary</span>(logit_roc)</span></code></pre></div>
<pre><code>##                            
##  Method used: empirical    
##  Number of positive(s): 668
##  Number of negative(s): 276
##  Area under curve: 0.7675</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="index.html#cb89-1" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb89-2"><a href="index.html#cb89-2" tabindex="-1"></a><span class="do">####### making predictions on validation set </span></span>
<span id="cb89-3"><a href="index.html#cb89-3" tabindex="-1"></a><span class="do">###############################################</span></span>
<span id="cb89-4"><a href="index.html#cb89-4" tabindex="-1"></a></span>
<span id="cb89-5"><a href="index.html#cb89-5" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> valid, <span class="at">type =</span> <span class="st">&#39;response&#39;</span>)</span>
<span id="cb89-6"><a href="index.html#cb89-6" tabindex="-1"></a>logit_roc <span class="ot">&lt;-</span> <span class="fu">rocit</span>(p_hat[,<span class="dv">1</span>], valid<span class="sc">$</span>Winner_A)</span>
<span id="cb89-7"><a href="index.html#cb89-7" tabindex="-1"></a><span class="fu">plot</span>(logit_roc) <span class="co">#chance line = randomly flipped a coin to assign 0&#39;s and 1&#39;s</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-44-6.png" width="672" /></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="index.html#cb90-1" tabindex="-1"></a><span class="fu">plot</span>(logit_roc)<span class="sc">$</span>optimal  <span class="co">#cutoff = optimal cutoff that maximizes Youden index </span></span></code></pre></div>
<pre><code>##     value       FPR       TPR    cutoff 
## 0.2978469 0.3157895 0.6136364 0.7216416</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="index.html#cb92-1" tabindex="-1"></a><span class="fu">summary</span>(logit_roc)</span></code></pre></div>
<pre><code>##                           
##  Method used: empirical   
##  Number of positive(s): 44
##  Number of negative(s): 19
##  Area under curve: 0.6543</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="index.html#cb94-1" tabindex="-1"></a>planetMARS <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(p_hat[,<span class="dv">1</span>]<span class="sc">&gt;</span><span class="fl">0.7355581</span> ,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb94-2"><a href="index.html#cb94-2" tabindex="-1"></a></span>
<span id="cb94-3"><a href="index.html#cb94-3" tabindex="-1"></a></span>
<span id="cb94-4"><a href="index.html#cb94-4" tabindex="-1"></a></span>
<span id="cb94-5"><a href="index.html#cb94-5" tabindex="-1"></a>test<span class="sc">$</span>espnMARS <span class="ot">&lt;-</span> <span class="fu">predict</span>(mars2, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&#39;response&#39;</span>)</span>
<span id="cb94-6"><a href="index.html#cb94-6" tabindex="-1"></a>test<span class="sc">$</span>winnerMARS <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(test<span class="sc">$</span>espnMARS[,<span class="dv">1</span>] <span class="sc">&gt;</span> <span class="fl">0.6996445</span>,<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb94-7"><a href="index.html#cb94-7" tabindex="-1"></a></span>
<span id="cb94-8"><a href="index.html#cb94-8" tabindex="-1"></a>test[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">38</span><span class="sc">:</span><span class="dv">41</span>)]</span></code></pre></div>
<pre><code>## # A tibble: 4,095 × 9
##     Year TeamA TeamB      A_Seed B_Seed espnXGB winnerXGB espnMARS[,&quot;1&quot;] winnerMARS
##    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;
##  1  2024 UConn Purdue          1      1   0.607         0          0.683          0
##  2  2024 UConn UNC             1      1   0.697         0          0.629          0
##  3  2024 UConn Houston         1      1   0.560         0          0.339          0
##  4  2024 UConn Iowa State      1      2   0.667         0          0.738          1
##  5  2024 UConn Arizona         1      2   0.597         0          0.833          1
##  6  2024 UConn Marquette       1      2   0.597         0          0.790          1
##  7  2024 UConn Tennessee       1      2   0.597         0          0.826          1
##  8  2024 UConn Illinois        1      3   0.707         1          0.540          0
##  9  2024 UConn Kentucky        1      3   0.847         1          0.899          1
## 10  2024 UConn Baylor          1      3   0.727         1          0.789          1
## # ℹ 4,085 more rows</code></pre>
</div>
<div id="areas-of-improvement" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Areas of Improvement<a href="index.html#areas-of-improvement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The performance of the 2024 March Madness MARS and XGBoost models led to this recommendations section, which is where I hope to build on my work for next season and improve the predictions.</p>
<p>First and most importantly, I want to find a way to model injuries. One interesting article I read was looking at a team’s percentage of season minutes that were lost due to injury. This is something that could easily be a continuous variable within the model. Link for reference: <a href="https://www.statswithsasa.com/2024/03/18/march-madness-injuries/" class="uri">https://www.statswithsasa.com/2024/03/18/march-madness-injuries/</a></p>
<p>Next, I would like to consider modeling margin of victory (MOV) as opposed to raw 1/0 wins and losses outputs. Perhaps modeling a continuous variable can open up the possibilities for different results.</p>
<p>Other variables I’d like to consider are something like a difference between two team’s seeeds and non-conference performance. This was especially noticeable this year with Clemson, who despite being very underrated before the tournament, ended up going on an Elite 8 run. They performed very well in non-conference games, but it went unnoticed due to some in-conference losses.</p>
<p>Lastly, I want to expand my test set beyond just one tournament’s year of data. This can help improve my models and serve as a form of cross validation so I don’t overfit to just one year.</p>
</div>
<div id="references" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> References<a href="index.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><strong>TeamRankings.com</strong> historical CBB team stats</p></li>
<li><p><strong>SportsReference.com</strong> historical CBB team stats</p></li>
<li><p><strong>BartTorvik.com</strong> historical CBB stats (especially efficiency metrics)</p></li>
<li><p><strong>KenPom.com</strong> did not use data but referenced methodology</p></li>
<li><p><strong><a href="https://rpubs.com/DenJackson/marchmadness" class="uri">https://rpubs.com/DenJackson/marchmadness</a></strong> helpful resource where I modeled my project after</p></li>
<li><p><strong><a href="https://www.printyourbrackets.com/images/marchmadness-2023.pdf" class="uri">https://www.printyourbrackets.com/images/marchmadness-2023.pdf</a></strong> historical bracket performance for any year</p></li>
</ul>
<p>Any web scraping code can be found in a separate R file in my Github.</p>
</div>
<div id="additional-code-appendix" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Additional Code Appendix<a href="index.html#additional-code-appendix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is the code for calculating the Pythagorean Expectation for a team’s expected win percentage. Pythagorean Expectation is the ratio of a team’s total points raised to the n power, over the sum of their total points to the n power plus their opponent’s total points to the n power. I ran a grid search on possible exponent values from 2 to 20 stepping by 0.1 to find the optimal exponent.</p>
<p>I calculated how far off the team’s win percentage was off the expected win percentage using mean absolute error (MAE). I summed the MAE for both team A and team B and sorted by the lowest distance. This was minimized at an exponent of 8.8.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="index.html#cb96-1" tabindex="-1"></a>mm <span class="ot">&lt;-</span> <span class="fu">read_xlsx</span>(<span class="st">&quot;march_madness.xlsx&quot;</span>)</span>
<span id="cb96-2"><a href="index.html#cb96-2" tabindex="-1"></a></span>
<span id="cb96-3"><a href="index.html#cb96-3" tabindex="-1"></a><span class="co">#remove 2023 to train on 2000 - 2022</span></span>
<span id="cb96-4"><a href="index.html#cb96-4" tabindex="-1"></a>mm <span class="ot">&lt;-</span> mm <span class="sc">%&gt;%</span></span>
<span id="cb96-5"><a href="index.html#cb96-5" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>Year <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">2023</span>))</span>
<span id="cb96-6"><a href="index.html#cb96-6" tabindex="-1"></a></span>
<span id="cb96-7"><a href="index.html#cb96-7" tabindex="-1"></a></span>
<span id="cb96-8"><a href="index.html#cb96-8" tabindex="-1"></a><span class="co">#grid searching all possible values from 2 to 20 by 0.1</span></span>
<span id="cb96-9"><a href="index.html#cb96-9" tabindex="-1"></a>num_list <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">2</span>,<span class="dv">20</span>,<span class="fl">0.1</span>)</span>
<span id="cb96-10"><a href="index.html#cb96-10" tabindex="-1"></a>dict <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Exponent =</span> <span class="fu">as.numeric</span>(), <span class="at">MAE_A =</span> <span class="fu">as.numeric</span>(), <span class="at">MAE_B =</span> <span class="fu">as.numeric</span>(), <span class="at">Diff =</span> <span class="fu">as.numeric</span>())</span>
<span id="cb96-11"><a href="index.html#cb96-11" tabindex="-1"></a></span>
<span id="cb96-12"><a href="index.html#cb96-12" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> num_list) {</span>
<span id="cb96-13"><a href="index.html#cb96-13" tabindex="-1"></a>  </span>
<span id="cb96-14"><a href="index.html#cb96-14" tabindex="-1"></a>  pythag_A <span class="ot">&lt;-</span> (mm<span class="sc">$</span>A_TotalPoints<span class="sc">^</span>i)<span class="sc">/</span>(mm<span class="sc">$</span>A_TotalPoints<span class="sc">^</span>i <span class="sc">+</span> mm<span class="sc">$</span>A_TotalOppPoints<span class="sc">^</span>i)</span>
<span id="cb96-15"><a href="index.html#cb96-15" tabindex="-1"></a>  pythag_B <span class="ot">&lt;-</span> (mm<span class="sc">$</span>B_TotalPoints<span class="sc">^</span>i)<span class="sc">/</span>(mm<span class="sc">$</span>B_TotalPoints<span class="sc">^</span>i <span class="sc">+</span> mm<span class="sc">$</span>B_TotalOppPoints<span class="sc">^</span>i)</span>
<span id="cb96-16"><a href="index.html#cb96-16" tabindex="-1"></a>  </span>
<span id="cb96-17"><a href="index.html#cb96-17" tabindex="-1"></a>  </span>
<span id="cb96-18"><a href="index.html#cb96-18" tabindex="-1"></a>  mae_A <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(mm<span class="sc">$</span>A_WLPct_PRE <span class="sc">-</span> pythag_A))</span>
<span id="cb96-19"><a href="index.html#cb96-19" tabindex="-1"></a>  mae_B <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(mm<span class="sc">$</span>B_WLPct_PRE <span class="sc">-</span> pythag_B))</span>
<span id="cb96-20"><a href="index.html#cb96-20" tabindex="-1"></a>  </span>
<span id="cb96-21"><a href="index.html#cb96-21" tabindex="-1"></a>  dict[<span class="fu">nrow</span>(dict)<span class="sc">+</span><span class="dv">1</span>,] <span class="ot">&lt;-</span> <span class="fu">c</span>(i, mae_A, mae_B, mae_A <span class="sc">+</span> mae_B)</span>
<span id="cb96-22"><a href="index.html#cb96-22" tabindex="-1"></a>}</span>
<span id="cb96-23"><a href="index.html#cb96-23" tabindex="-1"></a></span>
<span id="cb96-24"><a href="index.html#cb96-24" tabindex="-1"></a><span class="fu">head</span>(</span>
<span id="cb96-25"><a href="index.html#cb96-25" tabindex="-1"></a>dict <span class="sc">%&gt;%</span></span>
<span id="cb96-26"><a href="index.html#cb96-26" tabindex="-1"></a>  <span class="fu">arrange</span>(Diff) <span class="co">#8.8 is optimal</span></span>
<span id="cb96-27"><a href="index.html#cb96-27" tabindex="-1"></a>,<span class="dv">1</span>)</span></code></pre></div>
<pre><code>##   Exponent      MAE_A      MAE_B       Diff
## 1      8.8 0.04191466 0.04871111 0.09062577</code></pre>
<p>Here is all of the histograms of the variables in the model
<img src="bookdownproj_files/figure-html/unnamed-chunk-46-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-2.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-3.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-4.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-5.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-6.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-7.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-8.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-9.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-10.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-11.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-12.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-13.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-14.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-15.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-16.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-17.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-18.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-19.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-20.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-21.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-22.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-23.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-24.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-25.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-26.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-27.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-28.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-29.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-30.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-31.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-46-32.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="the-pool-of-tears.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tylerfarr5/march-madness/edit/main/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/tylerfarr5/march-madness/blob/main/index.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
